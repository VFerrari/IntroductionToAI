{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Logistic Regression",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDaiQ_rJqUTh"
      },
      "source": [
        "# **[MC906] Projeto Final**: Detecção de Desastres\n",
        "\n",
        "O objetivo desse projeto é construir e avaliar modelos de aprendizado de máquina que classifiquem quais Tweets são sobre desastres reais e quais não são.\n",
        "\n",
        "## **Acessar Diretório do Projeto**\n",
        "\n",
        "Esse Notebook assume que você está executando o código dentro da pasta `Projeto Final/Código`, que contém todos os códigos fontes relativos a esse trabalho. Para acessar o diretório no Colab é preciso criar um atalho em seu Drive (right click no diretório -> \"Adicionar atalho ao Google Drive\") e executar as células abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsbMgN6X25Rb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "c696b32e-a858-4752-c05d-211ea6294841"
      },
      "source": [
        "# Conectar ao Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k08yfRqb3L1z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "4ebdc861-8865-40dd-f86b-ad9f6b08b0f8"
      },
      "source": [
        "# Diretório do Projeto (/content/drive/My Drive/{path até Projeto Final/Código}), \n",
        "# dependendo da onde se localiza o atalho no seu Drive\n",
        "% cd '/content/drive/My Drive/[MC906] Introdução à Inteligência Artificial/Projeto Final/Código'\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1HmHC25ZqX3hUlCsRT-S0qiSsjwf10jLn/[MC906] Introdução à Inteligência Artificial/Projeto Final/Código\n",
            "'Attention CNN'\t\t\t Modelos\n",
            " BERT\t\t\t\t __pycache__\n",
            "'Convolutional Neural Network'\t'Quasi-Recurrent Networks'\n",
            "'Dense Neural Networks'\t\t'Recurrent Neural Networks'\n",
            "'Logistic Regression'\t\t tokenization.py\n",
            "'Melhor Pré-Processamento'\t utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARzfb7XfqS14"
      },
      "source": [
        "## **Dependências:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJpOFGnJqUT1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "118fb4b8-970d-43d3-b588-56ee2c9b50d3"
      },
      "source": [
        "# Imports de pacotes instalados\n",
        "from os.path import join\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Imports locais\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtbUy5zOFiIE"
      },
      "source": [
        "## **Dataset:**\n",
        "\n",
        "Utilizamos um *dataset* disponível no site [Kaggle](https://www.kaggle.com/c/nlp-getting-started/data) (em inglês). Cada tweet apresenta três atributos: seu conteúdo (`text`), uma palavra-chave (`keyword`, opcional) e a localização da onde foi enviado (`location`, opcional). Como só usaremos o texto, removemos os dois últimos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YRHWP2hqUUn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7c68db06-0cc8-4427-8fe6-3845761ac405"
      },
      "source": [
        "# Ler e limpar dados (removendo colunas do id, keyword e location)\n",
        "train = pd.read_csv(\"../Dataset/train.csv\")\n",
        "train = train.drop(['id','keyword','location'], axis=1)\n",
        "\n",
        "# Imprimir alguns dados\n",
        "print(train.head())\n",
        "vals = train.groupby('target').count()\n",
        "print(\"\\nSome General insights:\")\n",
        "print(f\"Figure of Speech: {vals.iloc[0]['text']*100/len(train):.2f}%\")\n",
        "print(f\"Actual Accidents: {vals.iloc[1]['text']*100/len(train):.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Some General insights:\n",
            "Figure of Speech: 57.03%\n",
            "Actual Accidents: 42.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnRVxA9WqUVo"
      },
      "source": [
        "## **Pré-processamento:**\n",
        "\n",
        "Aplicamos uma série de passos para o pré-processamento textual do dataset:\n",
        "* Remoção de pontuações, *stop words* e palavras menos frequentes.\n",
        "* Tokenization: Separação das frases em tokens/palavras e pontuações.\n",
        "* Stemming: corte de prefixos e sufixo das palavras de forma indiscriminada. Associa as palavras à uma raiz comum.\n",
        "* One Hot Encoding: Conversão dos valores categóricos em vetores binários."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1-n2Y3UqUVw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "b0678010-8ed4-4f8d-c0a9-ef053d81ed6c"
      },
      "source": [
        "# Limpar texto\n",
        "print(f\"Raw Tweet:\\n\\t\",train.text[1])\n",
        "train.text = train.text.apply(clean_up)\n",
        "print(\"\\nRemoved Punctuation and Special Chars:\\n\\t\", train.text[1])\n",
        "\n",
        "# Aplicar tokenização\n",
        "train.text = train.text.apply(word_tokenize)\n",
        "print(\"\\nTokenized Tweet:\\n\\t\", train.text[1])\n",
        "\n",
        "# Remover stop words e aplicar stemming\n",
        "show_wordcount(train.text, \"\\nBefore Removal:\")\n",
        "train.text = train.text.apply(stop_words)\n",
        "print(\"\\nRemoved Stop Words:\\n\\t\", train.text[1])\n",
        "train.text = train.text.apply(stemming)\n",
        "print(\"\\nStemmed Tweet:\\n\\t\",train.text[1])\n",
        "show_wordcount(train.text, \"\\nAfter Removal:\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw Tweet:\n",
            "\t Forest fire near La Ronge Sask. Canada\n",
            "\n",
            "Removed Punctuation and Special Chars:\n",
            "\t Forest fire near La Ronge Sask Canada\n",
            "\n",
            "Tokenized Tweet:\n",
            "\t ['Forest', 'fire', 'near', 'La', 'Ronge', 'Sask', 'Canada']\n",
            "\n",
            "Before Removal:\n",
            "Amount of Words: 105647\n",
            "Amount of Distinct Words: 21526\n",
            "\n",
            "Removed Stop Words:\n",
            "\t ['fire', 'near', 'Canada', 'Ronge', 'Sask', 'Forest', 'La']\n",
            "\n",
            "Stemmed Tweet:\n",
            "\t ['fire', 'near', 'canada', 'rong', 'sask', 'forest', 'la']\n",
            "\n",
            "After Removal:\n",
            "Amount of Words: 77292\n",
            "Amount of Distinct Words: 18502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErLoEZg7qUWF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "39e0919a-cb48-4f8c-f29a-6374d95a6824"
      },
      "source": [
        "show_wordcount(train.text, '\\nBefore Removal:')\n",
        "words, counts = count_words(train.text)\n",
        "\n",
        "# Criar lista de palavras pouco frequentes\n",
        "MIN_FREQ = 2\n",
        "discard = []\n",
        "for (w,c) in zip(words, counts[0]):\n",
        "    if c < MIN_FREQ : \n",
        "      discard.append(w)\n",
        "\n",
        "# Criar conjunto com as palavras selecionadas pelo countvectorizer\n",
        "# descartando as palavras pouco frequentes, e remover as palavras\n",
        "# do tweet que não pertençam a esse conjunto\n",
        "print(f\"\\nDiscarding {len(discard)} Words...\")\n",
        "select = set(words)-set(discard)\n",
        "train.text = train.text.apply(lambda x : list(set(x) & select))\n",
        "show_wordcount(train.text, '\\nAfter Removal:')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Before Removal:\n",
            "Amount of Words: 77292\n",
            "Amount of Distinct Words: 18502\n",
            "\n",
            "Discarding 13145 Words...\n",
            "\n",
            "After Removal:\n",
            "Amount of Words: 62736\n",
            "Amount of Distinct Words: 5320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew0zXVTnqUWv"
      },
      "source": [
        "# Aplicar One Hot Encoding \n",
        "labels = LabelEncoder().fit(list(select))\n",
        "encoded = train.text.apply(labels.transform) # Transforming words to labels\n",
        "encoded = encoded.apply(lambda x : onehotencoding(x, len(select)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVrv5TuiqUXO"
      },
      "source": [
        "# Array numpy dos tweets pré-processados e targets\n",
        "X = np.array(encoded.tolist())\n",
        "Y = np.array([[x] for x in train.target.tolist()]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctu3fTWTqUXl"
      },
      "source": [
        "# Dividir dataset\n",
        "X, X_v, Y, Y_v = train_test_split(X, Y, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAvU5QtkGZA3"
      },
      "source": [
        "## **Modelo**: Regressão Logística\n",
        "\n",
        "Para um resultado inicial de referência, foi testado o método mais simples de classificação estudado: a regressão logística. Utilizamos a classe \"LogisticRegression\", da biblioteca *Scikit-Learn*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtKCKE4OxXXV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "1a486404-7b1b-4b6f-bf06-96421f21c810"
      },
      "source": [
        "log = LogisticRegression(max_iter=1000)\n",
        "log_history = log.fit(X, Y)\n",
        "print(\"Training Acc: \", log.score(X,Y))\n",
        "print(\"Validation Acc: \", log.score(X_v,Y_v))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Acc:  0.9186980002919282\n",
            "Validation Acc:  0.7979002624671916\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
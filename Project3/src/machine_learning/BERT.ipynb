{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JkSYrV-yOKf"
      },
      "source": [
        "# **[MC906] Projeto Final**: Detecção de Desastres\n",
        "\n",
        "O objetivo desse projeto é construir e avaliar modelos de aprendizado de máquina que classifiquem quais Tweets são sobre desastres reais e quais não são.\n",
        "\n",
        "## **Acessar Diretório do Projeto**\n",
        "\n",
        "Esse Notebook assume que você está executando o código dentro da pasta `Projeto Final/Código`, que contém todos os códigos fontes relativos a esse trabalho. Para acessar o diretório no Colab é preciso criar um atalho em seu Drive (right click no diretório -> \"Adicionar atalho ao Google Drive\") e executar as células abaixo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPL34omFyb6J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "c1cbca8f-be6c-4a2c-c1f8-88b8827ded10"
      },
      "source": [
        "# Conectar ao Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoHJel042b8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "13943171-0489-4334-9460-e396679d033e"
      },
      "source": [
        "# Diretório do Projeto (/content/drive/My Drive/{path até Projeto Final/Código}), \n",
        "# dependendo da onde se localiza o atalho no seu Drive\n",
        "% cd '/content/drive/My Drive/[MC906] Introdução à Inteligência Artificial/Projeto Final/Código'\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1HmHC25ZqX3hUlCsRT-S0qiSsjwf10jLn/[MC906] Introdução à Inteligência Artificial/Projeto Final\n",
            "'Attention CNN'\t\t\t'Cópia de best_model_BERT_back.h5'\n",
            "'BERT Backup'\t\t\t Dataset\n",
            "'BERT Inicial.ipynb'\t\t'Dense Neural Networks'\n",
            "'BERT Melhor'\t\t\t Glove\n",
            " best_model_attention.h5\t'Logistic Regression and SVM'\n",
            " best_model_BERT_back.h5\t'Melhor Pré-Processamento'\n",
            " best_model_BERT.h5\t\t __pycache__\n",
            " best_model_CNN.h5\t\t'Quasi-Recurrent Networks'\n",
            " best_model_DNN.h5\t\t'Recurrent Neural Networks'\n",
            " best_model_RNN_glove.h5\t Relatório\n",
            " best_model_RNN.h5\t\t tokenization.py\n",
            " best_model_RNN_pool.h5\t\t utils.py\n",
            "'Convolutional Neural Network'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvOWA1ra2yHE"
      },
      "source": [
        "## **Dependências:**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JM3v5rE23oa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "1ce84c50-0e92-44c9-db79-f10ef9cbcb67"
      },
      "source": [
        "# Imports de pacotes instalados\n",
        "import pandas as pd\n",
        "from os.path import join, exists\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Activation, Dense, Input, Embedding\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Instalações\n",
        "!pip install sentencepiece # Usado em tokenization.py \n",
        "\n",
        "# Imports locais\n",
        "from utils import *\n",
        "import tokenization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5sbWDG03u8o"
      },
      "source": [
        "## **Dataset:**\n",
        "\n",
        "Utilizamos um *dataset* disponível no site [Kaggle](https://www.kaggle.com/c/nlp-getting-started/data) (em inglês). Cada tweet apresenta três atributos: seu conteúdo (`text`), uma palavra-chave (`keyword`, opcional) e a localização da onde foi enviado (`location`, opcional). Como só usaremos o texto, removemos os dois últimos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWr4aleO4J5V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "c329e218-4cbc-46f7-8eb1-1d0f9d2f79e5"
      },
      "source": [
        "# Ler e limpar dados (removendo colunas do id, keyword e location)\n",
        "train = pd.read_csv(\"../Dataset/train.csv\", dtype={'id': np.int16, 'target': np.int8})\n",
        "train = train.drop(['id','keyword','location'], axis=1)\n",
        "\n",
        "# Imprimir alguns dados\n",
        "print(train.head())\n",
        "vals = train.groupby('target').count()\n",
        "print(\"\\nSome General insights:\")\n",
        "print(f\"Figure of Speech: {vals.iloc[0]['text']*100/len(train):.2f}%\")\n",
        "print(f\"Actual Accidents: {vals.iloc[1]['text']*100/len(train):.2f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                text  target\n",
            "0  Our Deeds are the Reason of this #earthquake M...       1\n",
            "1             Forest fire near La Ronge Sask. Canada       1\n",
            "2  All residents asked to 'shelter in place' are ...       1\n",
            "3  13,000 people receive #wildfires evacuation or...       1\n",
            "4  Just got sent this photo from Ruby #Alaska as ...       1\n",
            "\n",
            "Some General insights:\n",
            "Figure of Speech: 57.03%\n",
            "Actual Accidents: 42.97%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MjgAruF55WJ"
      },
      "source": [
        "## **Pré-Processamento:**\n",
        "\n",
        "Inicialmente apenas removemos as pontuações e caracteres especiais de cada texto. Depois, com o próprio tokenizer do BERT (modelo que usamos nesse Notebook), normalizamos o texto e o dividimos em tokens, adequando o conjunto de dados para treinamento. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II4mixgy6ANl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f64d31a2-a32b-4b53-db29-eaa8b7002539"
      },
      "source": [
        "# Limpar texto\n",
        "print(f\"Raw Tweet:\\n\\t\",train.text[1])\n",
        "train.text = train.text.apply(clean_up)\n",
        "print(\"\\nRemoved Punctuation and Special Chars:\\n\\t\", train.text[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw Tweet:\n",
            "\t Forest fire near La Ronge Sask. Canada\n",
            "\n",
            "Removed Punctuation and Special Chars:\n",
            "\t Forest fire near La Ronge Sask Canada\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPdK9ixss7yT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "1f69e7f1-c087-455c-c0dc-2fd21ce9dedf"
      },
      "source": [
        "%%time\n",
        "# Carregar camada \n",
        "bert_layer = hub.KerasLayer(\n",
        "    'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2', \n",
        "    trainable=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 8.68 s, sys: 2.03 s, total: 10.7 s\n",
            "Wall time: 15.3 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNK9PlVptQWZ"
      },
      "source": [
        "# Salvar tokenizer\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGRy9CZV-OmR"
      },
      "source": [
        "def encode(texts, tokenizer, max_len=200):\n",
        "  ''' Função que aplica a tokenização WordPiece no conjunto `texts`, truncando\n",
        "  os textos em `max_len` caracteres e adicionando os tokens [CLS] e [SEP] em\n",
        "  seus extremos. '''\n",
        "\n",
        "  all_tokens = []\n",
        "  all_masks = []\n",
        "  all_segments = []\n",
        "  \n",
        "  for text in texts:\n",
        "      text = tokenizer.tokenize(text)\n",
        "          \n",
        "      text = text[:max_len-2]\n",
        "      input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "      pad_len = max_len - len(input_sequence)\n",
        "      \n",
        "      tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
        "      tokens += [0] * pad_len\n",
        "      pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
        "      segment_ids = [0] * max_len\n",
        "      \n",
        "      all_tokens.append(tokens)\n",
        "      all_masks.append(pad_masks)\n",
        "      all_segments.append(segment_ids)\n",
        "  \n",
        "  return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6EyqAvk0NdA"
      },
      "source": [
        "# Preparar textos para treinamento\n",
        "train_input = encode(train.text.values, tokenizer)\n",
        "train_labels = train.target.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnB8jP4XgL9H"
      },
      "source": [
        "## **Modelo**: BERT\n",
        "\n",
        "BERT é um modelo não-supervisionado e bidirecional pré-treinado em uma coleção de escritos enorme (Wikipedia + BookCorpus) que pode ser afinado (*fine-tuned*) para tarefas específicas. Por representar o estado da arte em NLP, resolvemos testar nesse projeto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_-ZCJVsgtpI"
      },
      "source": [
        "def NN(bert_layer, max_len=200):\n",
        "  ''' Função que constrói o modelo BERT. '''\n",
        "\n",
        "  input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "  input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "  segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
        "  _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "  clf_output = sequence_output[:, 0, :]\n",
        "  out = Dense(1, activation='sigmoid')(clf_output)\n",
        "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEdmDIRgkwVt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "e95c3157-2dd5-4ee2-e3be-ce45a913e9a1"
      },
      "source": [
        "# Construir e compilar modelo\n",
        "neural_network = NN(bert_layer)\n",
        "neural_network.summary()\n",
        "neural_network.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_word_ids (InputLayer)     [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_mask (InputLayer)         [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "segment_ids (InputLayer)        [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        [(None, 768), (None, 109482241   input_word_ids[0][0]             \n",
            "                                                                 input_mask[0][0]                 \n",
            "                                                                 segment_ids[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 768)]        0           keras_layer[0][1]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         tf_op_layer_strided_slice[0][0]  \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 109,483,009\n",
            "Non-trainable params: 1\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI-8-CnimDk_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "936d3ad0-e931-48a8-eca3-cf9a39188a13"
      },
      "source": [
        "# Treinar no dataset pré-processado\n",
        "callbacks = [ModelCheckpoint(monitor='val_loss', filepath='./Modelos/best_model_BERT.h5', save_best_only=True)]\n",
        "history = neural_network.fit(train_input, train_labels, batch_size=32, epochs=15, validation_split=0.1, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "215/215 [==============================] - 15297s 71s/step - loss: 0.4800 - accuracy: 0.7813 - val_loss: 0.5658 - val_accuracy: 0.7612\n",
            "Epoch 2/15\n",
            "215/215 [==============================] - 15341s 71s/step - loss: 0.3989 - accuracy: 0.8304 - val_loss: 0.3724 - val_accuracy: 0.8412\n",
            "Epoch 3/15\n",
            "177/215 [=======================>......] - ETA: 43:55 - loss: 0.3614 - accuracy: 0.8517"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFKe_Xfg2fO6"
      },
      "source": [
        "## **Avaliação**\n",
        "\n",
        "Utilizamos o conjunto de teste mergeado com as respostas vazadas para avaliar se o modelo generaliza bem pro problema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpFjEteoyO79"
      },
      "source": [
        "# Carregar modelo treinado\n",
        "if exists('./Modelos/best_model_BERT.h5'):\n",
        "    neural_network = load_model('./Modelos/best_model_BERT.h5', custom_objects={'KerasLayer':hub.KerasLayer})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vF3GGr0zihH"
      },
      "source": [
        "# Salvar conjunto de testes com as respostas\n",
        "test = pd.read_csv(\"../Dataset/test_with_targets.csv\", dtype={'id': np.int16, 'target': np.int8})\n",
        "test = test.drop(['id','keyword','location'], axis=1)\n",
        "\n",
        "# Aplicar pré-processamento\n",
        "test.text = test.text.apply(clean_up)\n",
        "test_input = encode(test.text.values, tokenizer)\n",
        "test_labels = test.target.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUbqLrTCzjI1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5c60757f-71d7-4c2e-afc3-d3d48173d41c"
      },
      "source": [
        "# Avaliar modelo\n",
        "neural_network.evaluate(x=test_input, y=test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102/102 [==============================] - 2197s 22s/step - loss: 0.4003 - accuracy: 0.8302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4002986252307892, 0.8302175998687744]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}
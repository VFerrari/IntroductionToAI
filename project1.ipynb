{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook dependencies\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 1 - Search-based solutions for static Pac-Man game**\n",
    "**Subject:** MC906/MO416 - Introduction to Artificial Intelligence \n",
    "\n",
    "**Authors:**\n",
    "\n",
    "    Daniel Helú Prestes de Oliveira - RA 166215\n",
    "    Eduardo Barros Innarelli        - RA 170161\n",
    "    Matheus Rotta Alves             - RA 184403\n",
    "    Victor Ferreira Ferrari         - RA 187890\n",
    "    Vinícius Couto Espindola        - RA 188115\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "> TODO List:\n",
    "- [X] Imports\n",
    "- [X] Description of used libraries.\n",
    "- [X] Add AIMA to notebook.\n",
    "- [X] Project description\n",
    "\n",
    "### **Project**\n",
    "  The project consists of a study of different search methods in goal-based solutions to a variation of the **Pac-Man** game. Five search methods were tested and will be discussed here, being:\n",
    "- Two **uninformed** search methods (Breadth-First Search, Depth-First Search);\n",
    "- Two **informed** search methods ($A^*$ Search, Greedy Best-First Search);\n",
    "- One **local search** method (Simulated Annealing).\n",
    "\n",
    "  Aside from the search methods, the modeling of the problem will also be discussed and tested, with characteristics such as:\n",
    "- State representations (multiple);\n",
    "- Set of actions;\n",
    "- Objective state test;\n",
    "- The path cost.\n",
    "\n",
    "The models with different methods will be tested in different environments, and discussed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Usage**\n",
    "This project uses the _AIMA_ Python library for a generic implementation of the different search methods. You (the reader) can use any way you want to include that library in this notebook, but the following cell adds the library to the Python path if it is in the same folder as this notebook (if you clone the GitHub repository and its submodules, it should already be there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = os.path.abspath('')\n",
    "sys.path.insert(0,f'{direct}/aima-python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other external libraries used by the project are _NumPy_ for better array manipulation (maze), _Pandas_ for data organization in testing, _PyGame_ for visualization/animation of solutions and _wrapt_timeout_decorator_ for timeout in time-intensive methods. The other imports are part of the Standard Python Library.\n",
    "\n",
    "The timeout decorator library and all other external libraries can be installed via _pip_. **The timeout decorator may not work as intended in Windows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygame as py\n",
    "from wrapt_timeout_decorator import timeout\n",
    "from search import breadth_first_graph_search, depth_first_graph_search, astar_search, greedy_best_first_graph_search, simulated_annealing_full\n",
    "from search import manhattan_distance, euclidean_distance, memoize, Problem\n",
    "\n",
    "# Python libraries\n",
    "import re, time\n",
    "from itertools import product as combine\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "np.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem**\n",
    "\n",
    "> TODO List:\n",
    "- [X] Problem description \n",
    "- [X] Problem modeling\n",
    "- [X] Search agent (motivation, API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation and General Information**\n",
    "Consider the Pac-Man video game. The problem at hand is a variation of the game. We're given a maze where each position can have one of the following: start (unique), goal (unique), enemy, dots, walls or empty spaces. The Pac-Man will begin the game in the start position and will try to reached the goal by only crossing through empty spaces and dots positions. In the event were Pac-Man reaches outside the map, it's position is \"wrapped around\" the map, meaning that it will appear on the oposite side of the map maintaining one of it's coodinates.\n",
    "When compared to the original game, some simplifications were made: \n",
    " - Ghosts do not move during the search (their initial position is their only position). \n",
    " - Berries were removed from the game, therefore Pac-Man has no countermeasure against ghosts.\n",
    "\n",
    "### **Details and Environment**\n",
    "**Environment:** Is the maze described by the problem.\n",
    "\n",
    "**Actuator:** The only action pacman is allowed to execute is moving to a neighbor cell in the maze given the problems restrictions.\n",
    "\n",
    "**Sensors:** We consider that pacman can \"see\" the entirety of the given maze.\n",
    "\n",
    "**Known:** The consquences to the environment given an action are completely predictable.\n",
    "\n",
    "**Environment Properties:**\n",
    " - **Fully Observable:** We can see the entire maze at all times.\n",
    " - **Deterministic:** The next state is entirely defined by the current state and a possible action on it.\n",
    " - **Static:** The only change that occurs within the maze is caused by Pacman himself, the ghosts do not act.\n",
    " - **Discrete:** The environment has a finite number of states, being a finite maze with finite resources.\n",
    " - **Single Agent:** The only agent within the problem is Pacman.\n",
    " - **Sequential:** Every step builds on the previous ones to traverse the state space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Modeling Decisions**\n",
    "  For every move Pac-Man takes, it will pay a **cost of 1 point**. However, if a dot is eaten, it will receive a **payment of 10 points**. The sum of all costs and payments will be considered the score achieved by Pacman when executing a certain path. The path cost is the opposite, so the relation is $score = -1 . pathcost$. Pac-Man can only move one position at a time, in the down, up, left and right directions. The nodes are expanded in that exact order. The goal has the same score/path cost than a dot, not being worth more for the score.\n",
    "  \n",
    "Based on the maze and general problem restrictions, we can seek the best feasible path for Pac-Man to reach the goal. In our case, \"best\" refers to the path in which we achieve the highest score. However, since goal-based agents are being used, the **primary goal** is to reach the final position. This distinction is important for some methods.\n",
    "\n",
    "To save RAM, the maze is stored as **bytes** (ASCII characters), instead of unicode chars.\n",
    "\n",
    "### **Different Models**\n",
    "  For such a problem, there are many models that could be made to solve it, with potentially different state spaces, solutions and solution quality levels for each. Some could also be more appropriate for some methods than others. For that reason, it was decided that not one model would be the best for every situation, and the quality of the analysis would be better with **two** models being used with **every** method. The chosen models directly change the state space, and indirectly change what is the priority in achieving the goal.\n",
    "  \n",
    "  The first model has each state being a tuple representing **only** the position of Pac-Man in the maze. So, each position of the maze can only be visited **once** in graph-related searches (which almost all methods used are based on). This results in a **quadratic** state space, with $O(nm)$ states for a $n\\times  m$ maze, which is good for the most high-complexity methods. This model also focuses more on **reaching the goal** than maximizing the score, since it does not consider paths that use the same position twice, in favor of eating more dots. This could waste a lot of potentially better results in the informed methods. The position control is made using a \"global\" maze as a class atribute in the problem.\n",
    "  \n",
    "  The second model has each state being a tuple with both the position of Pac-Man in the maze **and the current maze itself**. This model accepts the possiblity of visiting the same position in the maze twice **only if** the maze is different (a dot has been eaten somewhere since the last time). The state space is still finite, but **exponential**, so this could lead to very high memory usage and execution time in some methods. This model also lets the method decide the priority: maximizing the score or reaching the goal. Since the state includes the maze configuration, it can be used to try to maximize the score before heading to the goal by going through possibly the same position more than once. The maze in the state is stored as a **tuple of tuples**, since a state should be hashable and tuples are immutable.\n",
    "  \n",
    "  Other models were considered with less information about the maze inside a state, but for the scope of this project these two models were the ones used. In the \"Conclusions\" section, other alternatives are suggested for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem Model 1: State With Only the Coordintes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 30: PacProblemNoMaze.py\n",
    "class Problem1(Problem):\n",
    "    ''' Modeling the static Pac-Man game problem for search. '''\n",
    "    \n",
    "    def __init__(self, initial, goal, maze, heuristic = None):\n",
    "        ''' Initial State:\n",
    "            Tuple of 2 elements. (i,j) in maze.\n",
    "            Goal State:\n",
    "            Tuple of 2 elements. (i,j) in maze.\n",
    "            Maze:\n",
    "            NumPy array of BYTES (to save RAM)\n",
    "        '''\n",
    "        Problem.__init__(self, initial, goal)\n",
    "        self.maze = maze\n",
    "        self.heuristic = heuristic\n",
    "        self.visited = set()\n",
    "        self.explored = set()\n",
    "        self.repeated_states = 0\n",
    "        \n",
    "    def actions(self, state):\n",
    "        ''' A state is the index of the maze (tuple). \n",
    "            An action is a tuple of i,j with the direction to walk.\n",
    "        '''\n",
    "        if state[0]*100+state[1] in self.visited: \n",
    "            self.repeated_states += 1\n",
    "        self.visited = self.visited.union([state[0]*100+state[1]])\n",
    "        \n",
    "        actions = []\n",
    "        possible = [(1,0),(-1,0),(0,1),(0,-1)]\n",
    "        idx = state\n",
    "\n",
    "        # This is the new behavior of eating the points\n",
    "        # Eat point if needed\n",
    "        if self.maze[idx] == b'.':\n",
    "            self.maze[idx] = b' '\n",
    "        for action in possible:\n",
    "            nxt = list(map(sum, zip(idx,action)))\n",
    "            \n",
    "            # Check circling around maze. If < 0, negative indexing will do the job.\n",
    "            if nxt[0] == self.maze.shape[0]:\n",
    "                nxt[0] = 0\n",
    "            elif nxt[1] == self.maze.shape[1]:\n",
    "                nxt[1] = 0\n",
    "            nxt = tuple(nxt)\n",
    "            \n",
    "            # Check ghosts and walls.\n",
    "            if self.maze[nxt] not in [b'o', b'|', b'-']:\n",
    "                actions.append(action)\n",
    "        return actions\n",
    "\n",
    "    def goal_test(self, state):\n",
    "        ''' Check if the Pac-Man reaches its destination.'''\n",
    "        return state == self.goal\n",
    "\n",
    "    def result(self, state, action):\n",
    "        ''' The result of an action is to move to the next position, and eat the point if needed.'''\n",
    "        idx = state\n",
    "        \n",
    "        # Get next position.\n",
    "        nxt = list(map(sum, zip(idx,action)))\n",
    "        \n",
    "        # Circle around maze.\n",
    "        if nxt[0] == self.maze.shape[0]:\n",
    "            nxt[0] = 0\n",
    "        elif nxt[0] < 0:\n",
    "            nxt[0] = self.maze.shape[0]-1\n",
    "        elif nxt[1] == self.maze.shape[1]:\n",
    "            nxt[1] = 0\n",
    "        elif nxt[1] < 0:\n",
    "            nxt[1] = self.maze.shape[1]-1\n",
    "        \n",
    "        nxt = tuple(nxt)\n",
    "        self.explored = self.explored.union([nxt[0]*100+nxt[1]])\n",
    "        \n",
    "        return nxt\n",
    "    \n",
    "    def path_cost(self, c, state1, action, state2):\n",
    "        ''' 10 points if it eats a point, and minus 1 point per movement. '''\n",
    "        nxt = self.maze[state2]\n",
    "        # Goal is same as a point (for now)\n",
    "        if nxt == b'.' or nxt == b'?':\n",
    "            cost = c-10\n",
    "        else:\n",
    "            cost = c\n",
    "        return cost+1\n",
    "\n",
    "    def value(self, state):\n",
    "        ''' Value is the \"score\" for the state.'''\n",
    "        # Use euclidean distance as a heuristic\n",
    "        if self.heuristic == 'euclidean':\n",
    "            return -5*euclidean_distance(state, self.goal)\n",
    "\n",
    "        # Use manhatam distance as a heuristic\n",
    "        if self.heuristic == 'manhattan':\n",
    "            return -5*manhattan_distance(state, self.goal)\n",
    "\n",
    "        # Use manhatam sum value as a heuristic\n",
    "        if self.heuristic == 'manhattan_sum':\n",
    "            # Accumulate sum of manhattan distances to foods\n",
    "            md_sum = 0\n",
    "            for food_idx in np.argwhere(self.maze == '.'):\n",
    "                md_sum += manhattan_distance(food_idx, state)\n",
    "            \n",
    "            tenth = len(np.argwhere(self.maze == ''))*0.1\n",
    "            goal_dist = -1*tenth*manhattan_distance(state, self.goal)\n",
    "            \n",
    "            return md_sum + goal_dist\n",
    "\n",
    "        # Error if no heuristic defined\n",
    "        if not self.heuristic:\n",
    "            raise Exception(\"CHOOSE A HEURISTIC before executing\")\n",
    "        if callable(self.heuristic):\n",
    "            raise Exception(\"CHOOSE A HEURISTIC NAME before executing, not a function.\")\n",
    "        \n",
    "    def h(self, node):\n",
    "        ''' Heuristic for informed/local search methods '''\n",
    "        \n",
    "        assert self.heuristic != None, \"Heuristic must be set!\"\n",
    "        assert callable(self.heuristic), \"Heuristic must be a function!\"\n",
    "        \n",
    "        # Eat dot if needed\n",
    "        if self.maze[node.state] == b'.':\n",
    "            self.maze[node.state] = b' '\n",
    "        \n",
    "        # Need to receive maze to estimate\n",
    "        return self.heuristic(node, self.maze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem Model 2: State With Coordinates and Maze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# %load -r 29: PacProblem.py\n",
    "class Problem2(Problem):\n",
    "    ''' Modeling the static Pac-Man game problem for search. '''\n",
    "    \n",
    "    def __init__(self, initial, goal, heuristic = None):\n",
    "        ''' Initial State:\n",
    "            Tuple of 2 elements. 1-Initial maze. 2. (i,j) in maze.\n",
    "            Goal State:\n",
    "            Tuple of 2 elements. (i,j) in maze.\n",
    "        '''\n",
    "        Problem.__init__(self, initial, goal)\n",
    "        self.visited = set()\n",
    "        self.explored = set()\n",
    "        self.repeated_states = 0\n",
    "        self.heuristic = heuristic\n",
    "        \n",
    "    def actions(self, state):\n",
    "        ''' \n",
    "            A state is the current maze (tuple of tuples) and the agent index\n",
    "            in the maze (tuple). An action is a tuple of i,j with the direction \n",
    "            to walk.\n",
    "        '''\n",
    "        i,j = state[1]\n",
    "        if i*100+j in self.visited: \n",
    "            self.repeated_states += 1\n",
    "        self.visited = self.visited.union([state[0]*100+state[1]])\n",
    "\n",
    "        actions = []\n",
    "        possible = [(1,0),(-1,0),(0,1),(0,-1)]\n",
    "        tuple_maze, idx = state\n",
    "                \n",
    "        # Convert maze into a numpy array.\n",
    "        maze = np.array(tuple_maze)\n",
    "        \n",
    "        for action in possible:\n",
    "            nxt = list(map(sum, zip(idx,action)))\n",
    "                        \n",
    "            # Check circling around maze. If < 0, negative indexing will do the job.\n",
    "            if nxt[0] == maze.shape[0]:\n",
    "                nxt[0] = 0\n",
    "            elif nxt[0] < 0:\n",
    "                nxt[0] = maze.shape[0]-1\n",
    "            elif nxt[1] == maze.shape[1]:\n",
    "                nxt[1] = 0\n",
    "            elif nxt[1] < 0:\n",
    "                nxt[1] = maze.shape[1] - 1\n",
    "                \n",
    "            nxt = tuple(nxt)\n",
    "            \n",
    "            # Check ghosts and walls.\n",
    "            if maze[nxt] not in [b'o', b'|', b'-']:\n",
    "                actions.append(action)\n",
    "        \n",
    "        return actions\n",
    "\n",
    "    def goal_test(self, state):\n",
    "        ''' Check if the Pac-Man reaches its destination.'''\n",
    "        return state[1] == self.goal\n",
    "\n",
    "    def result(self, state, action):\n",
    "        ''' The result of an action is to move to the next position, and eat the point if needed.'''\n",
    "        tuple_maze, idx = state\n",
    "        \n",
    "        # Convert maze into a numpy array.         \n",
    "        maze = np.array(tuple_maze)\n",
    "        \n",
    "        # Get next position.\n",
    "        nxt = list(map(sum, zip(idx,action)))\n",
    "        \n",
    "        # Circle around maze.\n",
    "        if nxt[0] == maze.shape[0]:\n",
    "            nxt[0] = 0\n",
    "        elif nxt[0] < 0:\n",
    "            nxt[0] = maze.shape[0]-1\n",
    "        elif nxt[1] == maze.shape[1]:\n",
    "            nxt[1] = 0\n",
    "        elif nxt[1] < 0:\n",
    "            nxt[1] = maze.shape[1]-1\n",
    "        \n",
    "        nxt = tuple(nxt)\n",
    "        self.explored = self.explored.union([nxt[0]*100+nxt[1]])\n",
    "        \n",
    "        # Eat point if needed\n",
    "        if maze[nxt] == b'.':\n",
    "            maze[nxt] = b' '\n",
    "        return tuple(map(tuple, maze)), nxt\n",
    "    \n",
    "    def path_cost(self, c, state1, action, state2):\n",
    "        ''' 10 points if it eats a point, and minus 1 point per movement. '''\n",
    "        nxt = np.array(state1[0])[state2[1]]\n",
    "        \n",
    "        # Goal is same as a point (for now)\n",
    "        if nxt == b'.' or nxt == b'?':\n",
    "            cost = c-10\n",
    "        else:\n",
    "            cost = c\n",
    "        return cost+1\n",
    "\n",
    "    def value(self, state):\n",
    "        ''' Value is the \"score\" for the state.'''\n",
    "        return -1 * self.path_cost(None, None, None, state)\n",
    "    \n",
    "    def h(self, node):\n",
    "        ''' Heuristic for informed/local search methods '''\n",
    "        \n",
    "        assert self.heuristic != None, \"Heuristic must be set!\"\n",
    "        \n",
    "        # No need to receive maze to estimate, as it is stored in node.state\n",
    "        return self.heuristic(node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Solution Visualization**\n",
    "In order to create a nice way to visualize each search behaviours, we used PyGame library visual interface to draw the maze with the Pacman's path. This was achieved by transforming the matrix with the maze in a matrix which mapped the display pixels. For the creation and manipulation of such, we defined a class PacScreen which serves as an interface between the search agent and the screen.\n",
    "\n",
    "To create the pixels map we simply represent each position of the matrix with a offset defined as `px=28` in PacScreen. This allows us to have a `px` square block to draw the contents of each position. To reference the block, we simply use the matrix indexes of the position we want to draw and multiply then by `px`, this gives us the top-left vertex of the corresponding block. However, since matrices are indexed as `M[i,j]`, where `i` is the line and `j` the column, and the display is indexed with cartesian coordinates `(x,y)`, we must map the indexes `i` and `j` to `y` and `x`, respectively.\n",
    "\n",
    "The PacScreen class works through the following steps: \n",
    " * First we initialize the class by setting essential parameters such as: a copy of the maze to be used as reference, the size of the screen (considering how many pixels each block will hold), the Pacman's initial position and the goal position.\n",
    " * Once we've ran a search with the same maze in PacScreen, we create a sequence of indexes `(i,j)` which indicate the Pacman's path in the maze. This sequence is then passed to the `PacScreen.run()` function were the maze is drawn in each of the positions visited by Pacman.\n",
    " * Inside the `run()` fucnction, we fist draw the starting environment with `PacScreen.draw()`. Then, every `interval=0.005` seconds we update the display with the next position using `PacScreen.update()`. The `update()` function is a light weight version of `draw()` where we only update what is necessary.\n",
    " \n",
    "**Note:** PyGame provides funcions which allows us to draw circles, rectangle and polygons, these were used to represent the map contained in the matrix. The walls are blocks filled with blue rectangles, the ghosts are triangles, the goal is a red circle, the dots are white circles and Pacman is represented by the yellow circle.\n",
    "> TODO List:\n",
    "- [ ] Maybe put this after all the methods?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='images/display_transform.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 26:125 PacScreen.py\n",
    "\n",
    "black = (0,0,0)\n",
    "grey = (50,50,50)\n",
    "white = (255,255,255)\n",
    "red = (255,0,0)\n",
    "green = (0,255,0)\n",
    "blue = (0,0,200)\n",
    "yellow = (255,255,0)\n",
    "\n",
    "# Múltiple of 4 ples\n",
    "px = 28\n",
    "\n",
    "class PacScreen():\n",
    "\n",
    "    def __init__(self, maze):\n",
    "        self.maze = maze.copy()\n",
    "        self.size = (self.maze.shape[1]*px,self.maze.shape[0]*px)\n",
    "\n",
    "        self.pac = tuple(map(int, np.where(self.maze==b'!')))\n",
    "        self.goal = tuple(map(int, np.where(self.maze==b'?'))) \n",
    "\n",
    "    def draw(self, maze, pac, goal):\n",
    "        walls = (maze==b'|')\n",
    "        bars = (maze==b'-')\n",
    "        dots = (maze==b'.')\n",
    "        ghosts = (maze==b'o')\n",
    "        y_max,x_max = maze.shape\n",
    "\n",
    "        for i in range(y_max):\n",
    "            for j in range(x_max):\n",
    "                x = j*px\n",
    "                y = i*px\n",
    "                if walls[i][j]:\n",
    "                    py.draw.rect(self.disp, blue, (x,y,px,px))\n",
    "                if bars[i][j]:\n",
    "                    py.draw.rect(self.disp, white, (x,int(y+px/2-px/4),px,int(px/4)))\n",
    "                if dots[i][j]:\n",
    "                    x_c = int(x+px/2)\n",
    "                    y_c = int(y+px/2)\n",
    "                    rad = int(px/5)\n",
    "                    py.draw.circle(self.disp, white, (x_c,y_c), rad)\n",
    "                if ghosts[i][j]:\n",
    "                    a = (x,y+px)\n",
    "                    b = (x+px,y+px)\n",
    "                    c = (int(x+px/2),y)\n",
    "                    py.draw.polygon(self.disp, green, (a,b,c))\n",
    "        \n",
    "        x_c = int(pac[1]*px + px/2)\n",
    "        y_c = int(pac[0]*px + px/2)\n",
    "        rad = int(px/3)\n",
    "        py.draw.circle(self.disp, yellow, (x_c,y_c), rad)\n",
    "\n",
    "        x_c = int(goal[1]*px + px/2)\n",
    "        y_c = int(goal[0]*px + px/2)\n",
    "        rad = int(px/4)\n",
    "        py.draw.circle(self.disp, red, (x_c,y_c), rad)\n",
    "\n",
    "    def update(self, maze, pac):\n",
    "        # Erase old pacman\n",
    "        x = self.pac[1]*px\n",
    "        y = self.pac[0]*px\n",
    "        py.draw.rect(self.disp, black, (x,y,px,px))\n",
    "        \n",
    "        # Leave visited tag\n",
    "        x_c = int(self.pac[1]*px + px/2)\n",
    "        y_c = int(self.pac[0]*px + px/2)\n",
    "        rad = int(px/4)\n",
    "        py.draw.circle(self.disp, grey, (x_c,y_c), rad)\n",
    "\n",
    "        # Update and draw\n",
    "        self.pac = pac\n",
    "        x_c = int(self.pac[1]*px + px/2)\n",
    "        y_c = int(self.pac[0]*px + px/2)\n",
    "        rad = int(px/3)\n",
    "        py.draw.circle(self.disp, yellow, (x_c,y_c), rad)\n",
    "\n",
    "    def step(self, action):    \n",
    "        if self.maze[action] == b'.':\n",
    "            self.maze[action] = ' ' \n",
    "        self.update(self.maze, action)\n",
    "\n",
    "    def run(self, path, interval=0.005):\n",
    "        # Create window and draw\n",
    "        self.disp = py.display.set_mode(self.size)\n",
    "        self.disp.fill(black)\n",
    "        self.map = py.PixelArray(self.disp)\n",
    "        self.draw(self.maze, self.pac, self.goal)\n",
    "\n",
    "        # Animate\n",
    "        while True:\n",
    "            for event in py.event.get():\n",
    "                if event.type == py.QUIT:\n",
    "                    py.quit()\n",
    "                    return\n",
    "\n",
    "            if path:\n",
    "                self.step(path.pop(0))\n",
    "                py.display.update()\n",
    "            time.sleep(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Search Agent**\n",
    "\n",
    "### **Motivation**\n",
    "This project consists of different search solutions for the same few problem models, and most methods use the same data structures. In that sense, the logical step to take is to create a shared API for every method, so that the environment is the same for every method, standardizing testing and result analysis. This agent should be able to formulate any of the problem models, change problem variables/properties and provide an API for result analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **API**\n",
    "The SearchAgent class can be seen in the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 26: SearchAgent.py\n",
    "class SearchAgent:\n",
    "    def __init__(self, maze):\n",
    "        self.maze = maze\n",
    "        self.display = None\n",
    "        self.problem = None\n",
    "        self.solution = None\n",
    "        self.init = None\n",
    "        self.state_maze = False\n",
    "        self.heuristic = None\n",
    "    \n",
    "    def set_maze(self, maze):\n",
    "        self.maze = maze\n",
    "        if self.problem and not self.state_maze:\n",
    "            self.problem.maze = maze.copy()\n",
    "            \n",
    "    def set_heuristic(self, heuristic):\n",
    "        self.heuristic = heuristic\n",
    "        if self.problem:\n",
    "            self.problem.heuristic = heuristic\n",
    "\n",
    "    def find_positions(self):\n",
    "        ''' Find initial and goal positions in correctly made mazes.'''\n",
    "        init = np.where(self.maze == b'!')\n",
    "        goal = np.where(self.maze == b'?')\n",
    "\n",
    "        # If init is not defined.\n",
    "        if init[0].size == 0:\n",
    "            init = None\n",
    "        else:\n",
    "            init = init[0][0], init[1][0]\n",
    "        \n",
    "        # If goal is not defined.\n",
    "        if goal[0].size == 0:\n",
    "            goal = None\n",
    "        else:\n",
    "            goal = goal[0][0], goal[1][0]\n",
    "        return init, goal\n",
    "    \n",
    "    def formulate_problem(self, initial_pos, goal_pos, with_maze, goal_conditions):\n",
    "        ''' Formulates problem based on positions and if maze is in state or not.'''\n",
    "        self.init = initial_pos        \n",
    "        \n",
    "        # Conditions\n",
    "        assert all(self.maze[initial_pos] != t for t in goal_conditions), \"Initial position does not satisfy conditions!\"\n",
    "        assert all(self.maze[goal_pos] != t for t in goal_conditions), \"Goal does not satisfy conditions!\"\n",
    "    \n",
    "        if not with_maze:\n",
    "            # Problem with maze in state\n",
    "            self.problem = Problem1(initial_pos, goal_pos, self.maze.copy(), self.heuristic)\n",
    "        else:\n",
    "            # Problem with only Pac-Man position in state\n",
    "            initial_pos = (tuple(map(tuple, self.maze)), initial_pos)\n",
    "            self.problem = Problem2(initial_pos, goal_pos, self.heuristic)\n",
    "        \n",
    "        self.state_maze = with_maze\n",
    "    \n",
    "    def search(self, method, *args):\n",
    "        ''' Execute search (solve problem). '''\n",
    "        self.solution = method(self.problem, *args)\n",
    "    \n",
    "    def get_solution(self):\n",
    "        if isinstance(self.solution, tuple):\n",
    "            return self.solution\n",
    "        elif self.solution:\n",
    "            return self.solution.solution()\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def get_path(self):\n",
    "        if self.solution:\n",
    "            return self.solution.path()\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    def get_score(self):\n",
    "        if self.solution:\n",
    "            return -1*self.solution.path_cost\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_explored(self):\n",
    "        if self.problem:\n",
    "            return len(self.problem.explored)\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "    def get_visited(self):\n",
    "        if self.problem:\n",
    "            return len(self.problem.visited)\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "    def get_repeated(self):\n",
    "        if self.problem:\n",
    "            return self.problem.repeated_states\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "\n",
    "    def transform_path(self):\n",
    "        ''' Transforms a path of nodes to a path of positions. '''\n",
    "        pos = []\n",
    "        path = self.solution.path()\n",
    "        \n",
    "        # Transform to positions            \n",
    "        # If maze is in state\n",
    "        if self.state_maze:\n",
    "            for node in path:\n",
    "                pos.append(node.state[1])\n",
    "        else:\n",
    "            for node in path:\n",
    "                pos.append(node.state)\n",
    "        return pos\n",
    "    \n",
    "    def display_path(self, path, interval=0.005):\n",
    "        ''' Animate maze, to visualize found path. '''\n",
    "        self.display = PacScreen(self.maze)\n",
    "        self.display.run(path, interval)\n",
    "    \n",
    "    def apply_actions(self, actions):\n",
    "        ''' Apply actions to maze in ascii, to visualize found path. '''\n",
    "        direct_x = {1:b'>', -1:b'<'}\n",
    "        direct_y = {-1:b'^', 1:b'v'}\n",
    "        directions = {b'^', b'v', b'<', b'>'}\n",
    "        maze = self.maze.copy()\n",
    "        pos = self.init\n",
    "        \n",
    "        # Initial\n",
    "        maze[pos] = b'!'\n",
    "        for act in actions:\n",
    "            pos = list(map(sum, zip(pos,act)))\n",
    "            \n",
    "            # Circle around maze\n",
    "            if pos[0] == maze.shape[0]:\n",
    "                pos[0] = 0\n",
    "            elif pos[0] < 0:\n",
    "                pos[0] = maze.shape[0]-1\n",
    "            elif pos[1] == maze.shape[1]:\n",
    "                pos[1] = 0\n",
    "            elif pos[1] < 0:\n",
    "                pos[1] = maze.shape[1]-1\n",
    "            pos = tuple(pos)\n",
    "            \n",
    "            if maze[pos] in directions:\n",
    "                continue\n",
    "            \n",
    "            if act[0] != 0:\n",
    "                maze[pos] = direct_y[act[0]]\n",
    "            elif act[1] != 0:      \n",
    "                maze[pos] = direct_x[act[1]]\n",
    "        \n",
    "        # Goal\n",
    "        maze[pos] = b'?'\n",
    "        return maze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SearchAgent lets the user:\n",
    "- Find the initial and goal positions of a maze from a standard as seen in the next section;\n",
    "- Formulate any of the problem models, given an initial position, a goal position, goal conditions and flags indicating the model to use;\n",
    "- Change the maze without changing the problem;\n",
    "- Formulate a new problem with the same maze;\n",
    "- Use any search method from AIMA and store its return object;\n",
    "- Get the solution (sequence of actions or path);\n",
    "- Get the solution path;\n",
    "- Get the total ammount of node visited by the search;\n",
    "- Get the final score from the solution;\n",
    "- Visualize the solution in ASCII form, returning a NumPy array with the maze modified with the taken path;\n",
    "- Use an animation engine created in _pygame_ to animate the final path.\n",
    "\n",
    "Therefore, it accomplishes its objective of providing a generic environment to run tests and solve the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SearchAgent(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test Cases**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Maybe show two examples (one dense and it's correspondent sparse)\n",
    "\n",
    "For testing purposes, we generated 10 mazes using the [tool provided by classmate Gabriel Bomfim](https://gabomfim.github.io/pacman-mazegen/tetris/many.htm) in Google Classroom, which adapts the [maze generator](https://shaunlebron.github.io/pacman-mazegen/) linked in the project description. Each tile is represented by a char, where **|** and **-** are walls, **.** are foods and **o** are ghosts. For each maze, we choosed three start and goal positions, respectively symbolized by **!** and **?**.\n",
    "\n",
    "As this tool creates mazes filled with food, we thought that it would be good for comparision to also test sparse mazes, which we created by randomly removing dots in the dense ones. These variations, together with the originals, give us a total of 60 mazes, stored in `./mazes` directory.\n",
    "\n",
    "The tests were run with the SearchAgent class defined in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='images/test_sample.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 7:13 testing.py\n",
    "# Getting test files\n",
    "path = 'mazes/'\n",
    "sizes = ['dense/','sparse/']\n",
    "maze = ['1','2','3','4','5','6','7','8','9','10']\n",
    "pos = ['a','b','c']\n",
    "test_files = [path+s+i+l for (s,i,l) in list(combine(sizes,maze,pos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 14:120 testing.py\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()  # As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)\n",
    "\n",
    "def collect_data(data, out_path):\n",
    "    # Gather info by grouping dense/sparse and map\n",
    "    ids = data.groupby(['type','id'])\n",
    "    ids_means = ids.mean()\n",
    "    ids_means.name = 'ids_means'\n",
    "    ids_max = ids.max().drop(columns='class')\n",
    "    ids_max.name = 'ids_max'\n",
    "    ids_min = ids.min().drop(columns='class')\n",
    "    ids_min.name = 'ids_min'\n",
    "\n",
    "    # Gather info by grouping dense/sparse\n",
    "    types = data.groupby(['type'])\n",
    "    types_means = types.mean()\n",
    "    types_means.name = 'types_mean'\n",
    "    types_max = types.max().drop(columns='class')\n",
    "    types_max.name = 'types_max'\n",
    "    types_min = types.min().drop(columns='class')\n",
    "    types_min.name = 'types_min'\n",
    "\n",
    "    dataframes = [data,ids_means,ids_max,ids_min,types_means,types_max,types_min,]\n",
    "    if out_path:\n",
    "        for df in dataframes: df.to_csv(f'{out_path}/{df.name}.csv')\n",
    "                        \n",
    "    return dataframes\n",
    "    \n",
    "#### TESTING ROUTINE ####\n",
    "\n",
    "def run_tests(test_files, search, *args, repeat=1, out_path=''):\n",
    "    print(\"#### Starting New Test Routine ####\")\n",
    "    keys = ['type','id','class',f'time_{repeat}avg',f'cost_{repeat}avg',f'fails_{repeat}total','visited','repeated','explored']\n",
    "    data = pd.DataFrame(columns=keys)\n",
    "    data.name='all_data'\n",
    "    count,total = 0, repeat*len(test_files)\n",
    "    agent = None\n",
    "\n",
    "    for maze_file in test_files: \n",
    "        maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "        if not agent: agent = SearchAgent(maze) # Build agent if not yet created\n",
    "        \n",
    "        deltas = []\n",
    "        fails = []\n",
    "        costs = []\n",
    "        visits = []\n",
    "        repeated = []\n",
    "        explored = []\n",
    "\n",
    "        for i in range(1,repeat+1):\n",
    "            count += 1\n",
    "            progress(count, total, status=f\"{maze_file} X{i:4d}\")            \n",
    "            \n",
    "            # Set up new map\n",
    "            agent.set_maze(maze)                # Reset agent's maze\n",
    "            init,goal = agent.find_positions()  # Reset positions\n",
    "\n",
    "            # Run and time it\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                cost = search(agent, maze, init, goal, *args)\n",
    "            except TimeoutError:\n",
    "                cost = 0\n",
    "            finally:\n",
    "                tf = time.perf_counter()\n",
    "\n",
    "            # Data acumulators\n",
    "            deltas += [tf - t0]\n",
    "            fails += [0 if cost else 1]\n",
    "            costs += [0 if not cost else cost]\n",
    "            visits += [agent.get_visited()]\n",
    "            repeated += [agent.get_repeated()]\n",
    "            explored += [agent.get_explored()]\n",
    "\n",
    "        fails_ratio = sum(fails)/repeat\n",
    "        deltas_avg = sum(deltas)/repeat # Consider failures\n",
    "        if fails_ratio < 1:\n",
    "            cost_avg = sum(costs)/(repeat-sum(fails)) # Don't consider failures\n",
    "        else:\n",
    "            cost_avg = 0\n",
    "        visits_avg = sum(visits)/repeat\n",
    "        repeated_avg = sum(repeated)/repeat\n",
    "        explored_avg = sum(explored)/repeat\n",
    "\n",
    "        # Fetch info from file name\n",
    "        match = re.match(r'mazes/(\\w+)/(\\d+)(\\w)', maze_file)\n",
    "        maze_type = match.group(1)\n",
    "        maze_id = int(match.group(2))\n",
    "        maze_class = match.group(3)\n",
    "\n",
    "        # Add results to dataframe\n",
    "        values = [maze_type,maze_id,maze_class,deltas_avg,cost_avg,\n",
    "                  fails_ratio,visits_avg,repeated_avg,explored_avg]\n",
    "        data.loc[maze_file] = dict(zip(keys, values))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return collect_data(data, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Uninformed Search Methods**\n",
    "The Uninformed Search Methods, also known as Blind Search Methods, are algorithms that are given no information about the problem other than its definition. They are only able to generate possible successors of a state and analyze these sucessors in a sequence according to the algorithm applied. They analyze each successor created until it finds the goal state. Every state receives the same treatment, and every decision is local according to a method that can only use local information of the problem (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Breadth-First Search (BFS) Solution**\n",
    "###### **Responsible:** Victor\n",
    "\n",
    "> TODO List:\n",
    "- [X] Short theoretical introduction\n",
    "- [X] Run tests script with and without maze in state\n",
    "- [ ] Results table \n",
    "- [X] Analysis with relevant(s) animation(s)\n",
    "\n",
    "#### **The Method**\n",
    "Breadth-First Search (BFS) is an Uninformed Search method that visits every state in order of distance from the initial state. So this method expands every successor once it reaches a node, and then visits each one of them in a predetermined order in a queue (FIFO). In the _AIMA_ library, this is done in the order of actions given by the problem.\n",
    "\n",
    "BFS, for the Pac-Man problem, is a complete method, since the maze is finite and in the **graph** variation the method does not visit the same state twice. The graph variation is used because there can be loops in the state space, so a tree is not the best representation of it. This method always gets the shortest path to the goal position, but it is **not** an optimal method for the problem, because not all paths have the same cost, eating more dots will result in better solutions.\n",
    "\n",
    "This method is **exact and deterministic**, so it will **always** return the same solution to an instance. For that reason, there's no use in averaging results from different executions, and the method can be analyzed with only one execution.\n",
    "\n",
    "#### **Pre-Analysis**\n",
    "Since this method always expands the shallowest node, memory and time can be a big problem, so problems with less successors in each node and/or shallower trees can make a huge difference in these constraints. For that reason, between the two models created for the problem, the one that does not have the maze in its state is the likely best one for this method. \n",
    "\n",
    "Since BFS always finds the shortest path to the goal and always visits the shallowest node available, it will never need to check the same position twice. It will go through every position in the maze in order of proximity to the initial point, eventually getting to the goal without needing to cross a taken path. So the quadratic state space of the model with only the position as state will likely get the same answer as the exponential state space of the other model, but faster.\n",
    "\n",
    "And this increase in speed (and memory usage) can be pivotal, for this method can easily explode in time and memory with the more complex model. However, this likely behavior also hints that the result will not include a very high score, because the path cost is not considered at all in this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for Model Without Maze in State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_no_maze(agent, maze, init, goal, *args):\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.search(breadth_first_graph_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    bfs_no_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/bfs/problem1'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for Model With Maze in State**\n",
    "\n",
    "CAUTION: the below cell can take a really long time to run, and use a lot of RAM if timeout is too slow or not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(60)\n",
    "def bfs_with_maze(agent, maze, init, goal, *args):\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.search(breadth_first_graph_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    bfs_with_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/bfs/problem2'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis**\n",
    "As discussed in the pre-analysis, the diference in time requirements between both models is very high, for the exact same answers. This is a specific characteristic of BFS, since it always takes the shortest route, and it never crosses the same position twice. It was not possible to run every maze with BFS in the second model, because the execution time \"explodes\", as does the memory usage.\n",
    "\n",
    "The Pac-Man scores result for this method are not consistent, since BFS does not have any mechanisms to try and optimize the score, they are a byproduct of the shortest path found. There are instances where another direct path with the same length would result in a better score, as can be seen in an animation by running the cells below. Negative scores are usually the result of direct paths without dots to the goal. Except in those situations, the method had better scores in dense mazes with a far goal because the found path was more likely to have more dots.\n",
    "\n",
    "Analysing just the behavior of this method, isolated, it is clear that the model **without the maze** is the superior choice, with the same result but up to thousands of times faster and exploring/visiting less nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.genfromtxt('mazes/dense/2a', dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.search(breadth_first_graph_search)\n",
    "path = agent.transform_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.display_path(path, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Maze Animation**\n",
    "Running the cells below, a random maze can be solved with BFS and the first model (since it is better). An animation will run showcasing the solution (in a new window) and an ASCII representation of the solution will also be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.search(breadth_first_graph_search)\n",
    "path = agent.transform_path()\n",
    "sol = agent.apply_actions(agent.get_solution())\n",
    "print(sol.astype('<U1'))\n",
    "print(\"Score: \", agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.display_path(path, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Depth-First Search Solution**\n",
    "###### **Responsible:** Daniel\n",
    "\n",
    "> TODO List:\n",
    "- [X] Short theoretical introduction\n",
    "- [X] Run tests script with and without maze in state\n",
    "- [X] Results table \n",
    "- [ ] Analysis with relevant(s) animation(s)\n",
    "\n",
    "#### **The Method**\n",
    "The Depth-First Search(DFS) is an Uninformed Search method that expands its deepest node possible first, the one that has no sucessor. If the node is the goal state, it has found the path to the solution, otherwise it goes back to the most recent ancestor node that still has successor that were not expanded and it expands the deepest sucessor this ancestor node has.\n",
    "\n",
    "For the Pacman Problem the goal state considered for the problem was the final position that pacman should go. It did not consider analyzing all the possible outcomes of eating the dots to increase it points because the time and memory to consider all possible states would be exponential.\n",
    "\n",
    "The choosen search method between the two possible types of DFS inside the library was the **depth_first_graph_search**. The **depth_first_tree_search** implementation does not prevent a loop state as the pacman can return to the same position if its reach a dead end. The **depth_first_graph_search**, on the otherhand, avoids repeated states and redundant paths which can solve the problem of pacman getting caught in a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of the DFS using only the indexes as state**\n",
    "\n",
    "As first approach, we used only the position of the Pacman as part of the state of the problem. After running the code we can see the tables to view the time it took to calculate the solution for each maze and the number of points Pacman scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_no_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers DFS method and returns the score '''\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.search(depth_first_graph_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    dfs_no_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/dfs/nomaze'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of the DFS using the map and indexes as state**\n",
    "\n",
    "In the second approach we also considered the maze including its dots as part of the state. After running the code we can see the tables to view the time it took to calculate the solution for each maze and the number of points Pacman scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_with_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers DFS and returns the score '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.search(depth_first_graph_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    dfs_with_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/dfs/maze'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Maze Animation**\n",
    "Running the cells below, a random maze can be solved with DFS and the first model (since it is faster). An animation will run showcasing the solution (in a new window) and an ASCII representation of the solution will also be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.search(depth_first_graph_search)\n",
    "path = agent.transform_path()\n",
    "sol = agent.apply_actions(agent.get_solution())\n",
    "print(sol.astype('<U1'))\n",
    "print(agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.display_path(path, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Informed Search Methods**\n",
    "\n",
    "> TODO List:\n",
    "- [X] Short theoretical introduction\n",
    "- [X] Heuristics\n",
    "\n",
    "In contrast to the uninformed search methods, the agent in an informed search makes decisions based on additional knowledge about the search space. For this it has to evaluate which is the most promising path to take when it enters a state, which it does by using its knowledge to estimate how far a node is from the goal. Thus, specially for large spaces, the goal can be reached both in less time and more efficiently, depending on the estimation quality. In this project, we were requested to implement two of these evaluations, also known as heuristics.\n",
    "\n",
    "### **Heuristics**\n",
    "\n",
    "A common approach in pathfinding problems is to use the **Manhattan distance** as a heuristic, defined as the distance between the agent and the goal positions measured along axes at right angles (i.e., $|x_1 - x_2| + |y_1 - y_2|$, given that the agent is in $(x_1, y_1)$ and the goal is to reach $(x_2, y_2)$). In our problem, though, it's not a good decision to try to estimate a good cost to goal without taking into account the current configuration knowledge (the combinations of foods can make any estimation over the initial maze very far from optimal). Considering this, and keeping it simple in terms of code, we implemented the following heuristics:\n",
    "\n",
    " - Sum of Manhattan distances between the agent and all the foods, as it's highly possible that most of them will be eaten in the optimal path. \n",
    " - Minimum distance between the agent and a food, as the agent will likely approach the nearest food.\n",
    " \n",
    "Notice that both overestimate the optimal - it's not true that all the foods will be eaten in the best path neither that the nearest food will increase the agent final score. As overestimating heuristics, they break admissibility - that is, there is no guarantee that the optimal path will be found in the informed search algorithms. Even so, as our problem gives a high score to Pac-Man when it eats, we chose both expecting good paths to be found in reasonable running times.\n",
    "\n",
    "We also take the goal into account as a \"food\" in these heuristics after a certain depth, to avoid some situations in sparse mazes where the agent thinks it's better to pick a food far from it's position than going directly to goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_sum(node, maze = None):\n",
    "    ''' sum of manhattan distances between Pac-Man and all foods in maze '''\n",
    "    \n",
    "    # Detach maze configuration (if in state) and Pac-Man position\n",
    "    # TODO: there must be a better way.\n",
    "    if not isinstance(maze, np.ndarray):\n",
    "        tuple_maze, idx = node.state\n",
    "    else:\n",
    "        idx = node.state\n",
    "        tuple_maze = None\n",
    "    \n",
    "    # Convert tuple maze into a numpy array\n",
    "    maze = np.array(tuple_maze) if tuple_maze else maze\n",
    "    \n",
    "    # Take goal into account after a certain depth\n",
    "    if node.depth < maze.shape[0]*maze.shape[1]*(3/4):\n",
    "        foods_cond = maze == b'.'\n",
    "    else:\n",
    "        foods_cond = np.logical_or(maze == b'.', maze == b'?')\n",
    "    \n",
    "    # Accumulate sum of manhattan distances to foods\n",
    "    md_sum = 0\n",
    "    for food_idx in np.argwhere(foods_cond):\n",
    "        md_sum += manhattan_distance(food_idx, idx)\n",
    "    \n",
    "    # If no more dots\n",
    "    if md_sum == 0:\n",
    "        md_sum += manhattan_distance(np.argwhere(maze == b'?')[0], idx)\n",
    "    \n",
    "    return md_sum\n",
    "\n",
    "def manhattan_min(node, maze = None):\n",
    "    ''' minimum distance between the agent and a food '''\n",
    "    \n",
    "    # Detach maze configuration (if in state) and Pac-Man position\n",
    "    # TODO: there must be a better way\n",
    "    if not isinstance(maze, np.ndarray):\n",
    "        tuple_maze, idx = node.state\n",
    "    else:\n",
    "        idx = node.state\n",
    "        tuple_maze = None\n",
    "    \n",
    "    # Convert tuple maze into a numpy array\n",
    "    maze = np.array(tuple_maze) if tuple_maze else maze\n",
    "    \n",
    "    # Take goal into account after a certain depth\n",
    "    if node.depth < maze.shape[0]*maze.shape[1]*(3/4):\n",
    "        foods_cond = maze == b'.'\n",
    "    else:\n",
    "        foods_cond = np.logical_or(maze == b'.', maze == b'?')\n",
    "    \n",
    "    # List all manhattan distances between the agent and the foods\n",
    "    manhattan_distances = [\n",
    "        manhattan_distance(food_idx, idx) \n",
    "        for food_idx \n",
    "        in np.argwhere(foods_cond)\n",
    "    ]\n",
    "    \n",
    "    # If no more dots.\n",
    "    if len(manhattan_distances) == 0:\n",
    "        manhattan_distances = [\n",
    "            manhattan_distance(food_idx, idx) \n",
    "            for food_idx \n",
    "            in np.argwhere(maze == b'?')\n",
    "        ]\n",
    "    \n",
    "    # Get lowest of them\n",
    "    md_min = min(manhattan_distances)\n",
    "    return md_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A* Search Solution**\n",
    "###### **Responsible:** Eduardo\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)\n",
    "\n",
    "As an informed search algorithm, A\\* considers information about the exact path cost $g(n)$ from starting node to $n$ together with an heuristic $h(n)$ to estimate the total cost to goal, proceeding to the neighbor that gives the lowest $f(n) = g(n) + h(n)$.\n",
    "\n",
    "TODO falar que é mt usado em jogos, flexivel e blablabla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for problem without maze in state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NÃO RODA TÁ DESATUALIZADO\n",
    "\n",
    "def astar_pathcost_p1(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns path cost '''\n",
    "    agent.formulate_problem(init, goal, False, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(astar_heuristic_p1, True)\n",
    "    agent.search(astar_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    astar_pathcost_p1, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/astar/problem1'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for problem with maze in state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NÃO RODA TÁ DESATUALIZADO\n",
    "\n",
    "def astar_pathcost_p2(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns path cost '''\n",
    "    agent.formulate_problem(init, goal, True, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(astar_heuristic_p2, False)\n",
    "    agent.search(astar_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests, \n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        astar_pathcost_p2, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        out_path='data/astar/problem2'\n",
    "        )\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Greedy Best-First Search (GS) Solution**\n",
    "###### **Responsible:** Victor and Eduardo\n",
    "\n",
    "> TODO List:\n",
    "- [X] Short theoretical introduction\n",
    "- [X] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)\n",
    "\n",
    "#### **The Method**\n",
    "\n",
    "Greedy Best-First Search, or Greedy Search (GS) is an informed search method that visits a state based on a greedy criterion that tries to decide the best path to follow. This greedy criterion is determined by the chosen heuristic, and **only** it. \n",
    "\n",
    "So the difference between GS and $A^*$ is that, while the latter uses the **path cost** along with the heuristic value, the former only considers the heuristic value. Aside from that, the implementation of both methods in _AIMA_ are exactly the same.\n",
    "\n",
    "GS, for the Pac-Man problem, is a complete method, since the maze is finite and the represents the problem in **graph** form, so the method does not visit the same state twice. The graph variation is used because there can be loops in the state space, so a tree is not the best representation of it. It is **not optimal**, though, for the greedy decision might not lead to the best overall solution, which depends on the distribution of dots and goal position.\n",
    "\n",
    "Since the method doesn't expand nodes out of the way of the solution, the time (and therefore, memory) complexity is, in practice, lower than the complexity of the uninformed methods, but in the worst case it can be reduced to DFS, since it returns after finding a dead-end.\n",
    "\n",
    "This method is **deterministic**, so it will **always** return the same solution to an instance. For that reason, there's no use in averaging results from different executions, and the method can be analyzed with only one execution.\n",
    "\n",
    "The _AIMA_ library does not have a wrapper for this method, unlike for $A^*$, to help with heuristic organization and first memoization (_problem.h_ or parameter h). For that reason, a wrapper quite like the one for $A^*$ in _AIMA_ was made, and is available in the below cell.\n",
    "\n",
    "#### **Pre-Analysis**\n",
    "SE ACHAR MELHOR, TROCAR PARA EMBAIXO DOS RESULTS.\n",
    "\n",
    "Since GS does not include the path cost in its choice, it will have less issues going through empty spaces, which will make its solution potentially different from $A^*$, but not necessarily better or worse. In other aspects, it will likely have the same behavior as the previous method, going after most dots in the map before going to the goal, trying to optimize the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfs_wrapper(problem, h=None):\n",
    "    h = memoize(h or problem.h, 'h')\n",
    "    return greedy_best_first_graph_search(problem, lambda n: h(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for problem without maze in state**\n",
    "Both heuristics will be used in both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfs_min_p1(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns path cost '''\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(manhattan_min)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests, \n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_min_p1, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        out_path='data/gfs/problem1/min/'\n",
    "        )\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfs_sum_p1(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns path cost '''\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(manhattan_sum)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests, \n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_sum_p1, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        out_path='data/gfs/problem1/sum/'\n",
    "        )\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for problem with maze in state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfs_min_p2(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns path cost '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(manhattan_min)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests, \n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_min_p2, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        out_path='data/gfs/problem2/min/'\n",
    "        )\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfs_sum_p2(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns path cost '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(manhattan_sum)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests, \n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_sum_p2, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        out_path='data/gfs/problem2/sum/'\n",
    "        )\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis**\n",
    "- TODO: depends on the results.\n",
    "- TODO: add animation of maze 1a dense with manhattan sum - awful behavior.\n",
    "- TODO: 1a com manhattan min fica muito lento.\n",
    "\n",
    "...\n",
    "\n",
    "The _manhattan sum_ heuristic causes some peculiar behavior in the second model, which showcases the main difference with the $A^*$ search: the absence of the path cost in the value used to choose the next step. The heuristic in question uses the sum of manhattan distances to each dot in the maze, so it identifies the largest cluster of dots left. Since, in GFS, the path cost is not added to that value, and the model allows visiting the same position twice or more, the method does not have any issue in going through a lot of empty spaces to get to the largest cluster, and for that reason can go back and forth between two dot clusters of same size every time it eats a dot. This lowers the score significantly, and makes for a bad heuristic for the method. An example of this can be seen by running the cells below.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.genfromtxt('mazes/dense/1a', dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "agent.set_heuristic(manhattan_sum)\n",
    "agent.search(gfs_wrapper)\n",
    "path = agent.transform_path()\n",
    "print(agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.display_path(path, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Maze Animation**\n",
    "TODO: VERIFICAR\n",
    "\n",
    "Running the cells below, a random maze can be solved with GFS using the first model and the _manhattan min_ heuristic, since it gave the best results. An animation will run showcasing the solution (in a new window). To change models, just change the _True_ flag in the _formulate_problem_ function call. To change heuristic, just change the argument in the _set_heuristic_ function call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.set_heuristic(manhattan_sum)\n",
    "agent.search(gfs_wrapper)\n",
    "path = agent.transform_path()\n",
    "print(agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.display_path(path, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Local Search Methods**\n",
    "Local search is one of many heuristic based methods for optmization problems. They work by exploring the search space taking its huristic in consideration, the solution is given by memorizing the positions visited when executing such exploration. Usually, the search is done on feasible solutions, but, in our case, the search is used in a manner which it helps us find the feasible solution by iterating throught several intermediate states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simulated Annealing (SA) Solution**\n",
    "###### **Responsible:** Vinicius\n",
    "Simulated Annealing resembles Hillclimbing, but it enchances it to avoid local optimums. It does so by allowing steps which contradict the huristics decision: in hillclimbing we always take the best local decision hoping it will lead us to a global optimum, in simulated annealing we allow (with a probability which decreases over time) the search to take locally worst steps hoping it will allow us to scape local optimums. \n",
    "\n",
    "AIMA's implementation chooses at random the next node from a list of possible neighboors. Overall, it first selects a possible neighbor at random, then it evaluates the neighboors quality using the diference between the heuristic result for the current state and the neighbooring state. If the neighboor presents itself as a better state, the step is certainly taken, however, if is a worst state, the step is taken with a given probability `p`.\n",
    "\n",
    "Simulated Annealing is a **stochastic method**. It **does not ensure completeness** as it can re-visit states some states and never visit others. The method also does not ensure optmality of a found path.\n",
    "\n",
    "#### **Heuristics**\n",
    "We choose several heuristics to be used with this method.\n",
    "\n",
    "**Euclidean Distance to Goal:** it consists of calculating the euclidean distance of a state to the goal. We calculate the diference between the distance from the current state to the goal and the distance from the next state to the goal. If the distance on the next state is smalller the current state, then the next state is better.\n",
    "\n",
    "**Manhatam Distance to Goal:** in this case, we'll also use the diference between distances to the goal as a heuristic, but the manhatam distance is caluclated using the horizontal and vertical distance $(|Xa - Xb| + |Yz - Yb|)$. Then the same as before applies: smaller the distance to the goal, the better the state is considered.\n",
    "\n",
    "**Manhatam Sum to Dots (MSD):** in order to have a function which is not based on the goal, we'll also test the MSG heuristic. In this case, we calculate the sum of distances between the current state and all states which contain a dot. If the next state is has a smaller sum (is overall closer to the dots), then it's considered a better state.\n",
    "\n",
    "To use the Path Cost heuristic we must keep the state cost as part of the state it self, so the PacProblemCarryCost was used to test this heuristic. The remaining heuristics only requires the coordinates of the current position and the goal position, so the PacProblemNoMaze was used for a lightweight memory approach. Although, for the distance method wo require a post processing step to calculate the achieved cost given the path found.\n",
    "\n",
    "#### **Temperature Function Parameters**\n",
    "\n",
    "The function responsible for controling the probability `p` takes three parameters: the initial temperature `K`, the rate of cooling `lamb` and the maximum ammount of steps `limit`.\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Talk about the choice for the parameters in the cooling function\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results**\n",
    "##### **Path Cost Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimulatedAnnealing import annealing\n",
    "\n",
    "annealing.choose = \"pathcost\"\n",
    "\n",
    "def simulated_annealing_euclidean(agent, maze, init, goal, *args):\n",
    "    agent.formulate_problem((init,0), goal, False, True, [])\n",
    "    agent.search(annealing, maze, goal)\n",
    "    return agent.get_solution()[1]\n",
    "\n",
    "dataframes = run_tests(   test_files, \n",
    "                          simulated_annealing_euclidean, \n",
    "                          [], \n",
    "                          repeat=100)\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Euclidean Distance Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimulatedAnnealing import annealing\n",
    "\n",
    "annealing.choose = \"euclidean\"\n",
    "\n",
    "def simulated_annealing_euclidean(agent, maze, init, goal, *args):\n",
    "    agent.formulate_problem((init,0), goal, False, True, [])\n",
    "    agent.search(annealing, maze, goal)\n",
    "    return agent.get_solution()[1]\n",
    "\n",
    "dataframes = run_tests(   test_files, \n",
    "                          simulated_annealing_euclidean, \n",
    "                          [], \n",
    "                          repeat=100)\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Manhatam Distance Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SimulatedAnnealing import annealing\n",
    "\n",
    "annealing.choose = \"Manhatam\"\n",
    "\n",
    "def simulated_annealing_euclidean(agent, maze, init, goal, *args):\n",
    "    agent.formulate_problem((init,0), goal, False, True, [])\n",
    "    agent.search(annealing, maze, goal)\n",
    "    return agent.get_solution()[1]\n",
    "\n",
    "dataframes = run_tests(   test_files, \n",
    "                          simulated_annealing_euclidean, \n",
    "                          [], \n",
    "                          repeat=100)\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Maze Animation**\n",
    "TODO: ESCREVER DIREITO\n",
    "\n",
    "Running the cell below, a random maze can be solved with simulated annealing using the first model and the ??? heuristic, since it gave the best results. An animation will run showcasing the solution (in a new window). To change models, just change the _True_ flag in the _formulate_problem_ function call. To change heuristic, just change the argument in the _set_heuristic_ function call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.set_heuristic('manhattan')\n",
    "agent.search(annealing)\n",
    "path = agent.transform_path()\n",
    "print(agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.display_path(path, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparisions**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Compare methods and problems\n",
    "- [ ] Maybe display a graph comparing scores\n",
    "- [ ] Maybe animate one maze with all methods running (different colors to distinguish agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "### Methods\n",
    "After comparing every method, the one that performed in terms of solution cost, time/memory requirements and failures was the $A^*$ method. This result makes sense, since an informed method with good heuristics can maximize the score more easily than other types, and using the path cost with the heuristic value guarantees that the number of steps is taken into account. For those reasons, $A^*$ is often used for 2D pathfinding problems, even more than the other methods.\n",
    " \n",
    " [ ] TODO: verificar\n",
    "...\n",
    "\n",
    "### Future Work\n",
    "As mentioned near the beginning of this report, the two models created are not the only ways to model the problem. As examples, other models could include adding the amount of dots left in the maze instead of the entire maze, keeping the maze as a problem attribute, or adding a maze/amount of dots in the goal state so that another specific task is achieved, such as eat all the dots in the maze (the goal state would be the position and an empty maze, or 0), and the list keeps going.\n",
    "\n",
    "Future work could include analysing those other modeling ideas, and **check memory usage more precisely**, for an even better discussion. Online methods and genetic algorithm-based solutions could also be implemented, since they were not even touched upon in this project.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

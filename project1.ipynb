{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 1 - Search-based solutions for static Pac-Man game**\n",
    "**Subject:** MC906/MO416 - Introduction to Artificial Intelligence \n",
    "\n",
    "**Authors:**\n",
    "\n",
    "    Daniel Helú Prestes de Oliveira - RA 166215\n",
    "    Eduardo Barros Innarelli        - RA 170161\n",
    "    Matheus Rotta Alves             - RA 184403\n",
    "    Victor Ferreira Ferrari         - RA 187890\n",
    "    Vinícius Couto Espindola        - RA 188115\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Problem description \n",
    "- [ ] Problem modeling\n",
    "- [ ] Search agent (motivation, API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test Cases**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Maybe show two examples (one dense and it's correspondent sparse)\n",
    "\n",
    "For testing purposes, we generated 10 mazes using the [tool provided by classmate Gabriel Bomfim](https://gabomfim.github.io/pacman-mazegen/tetris/many.htm) in Google Classrom, which adapts the [maze generator](https://shaunlebron.github.io/pacman-mazegen/) linked in the project description. Each tile is represented by a char, where **|** and **-** are walls, **.** are foods and **o** are ghosts. For each maze, we choosed three start and goal positions, respectively symbolized by **!** and **?**.\n",
    "\n",
    "As this tool creates mazes fully filled with food, we thought that it would be good for comparision to also test sparse mazes, which we created by randomly removing dots in the dense ones. These variations, together with the originals, give us a total of 60 mazes, stored in `./mazes` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mazes.get_mazes import get_mazes\n",
    "denses, sparses = get_mazes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Uninformed Search Methods**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Breadth-First Search Solution**\n",
    "###### **Responsible:** Victor\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table \n",
    "- [ ] Analysis with relevant(s) animation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Depth-First Search Solution**\n",
    "###### **Responsible:** Daniel\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table \n",
    "- [ ] Analysis with relevant(s) animation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Informed Search Methods**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A* Search Solution**\n",
    "###### **Responsible:** Eduardo\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)\n",
    "\n",
    "As an informed search algorithm, A\\* takes into account information about the path cost together with an heuristic to evaluate which is the most promising path to take when it enters a state. For this evaluation, A\\* chooses in state $n$ to proceed to the neighbor that gives the lowest $f(n) = g(n) + h(n)$, $g(n)$ being the exact path cost from starting state to $n$ and $h(n)$ the heuristic estimated cost from $n$ to goal state.\n",
    "\n",
    "#### **Heuristic**\n",
    "\n",
    "How fast the agent reaches the goal in A\\* depends on the heuristic implemented and how it affects the nodes expansion. Without the maze in state this isn't that much of a concern, as for a $n\\times m$ maze there can be up to $O(nm)$ states. It's pretty hard, though, to estimate a good cost to goal without the current configuration knowledge (the combinations of foods can make any estimation over the initial maze very far from optimal). To simplify, we use the **Manhattan distance** as a heuristic for this variation of the problem, which is the distance between the agent and the goal positions measured along axes at right angles (i.e., $|x_1 - x_2| + |y_1 - y_2|$, given that the agent is in $(x_1, y_1)$ and the goal is to reach $(x_2, y_2)$). It's a admissible heuristic as there can't be a shorten path from node to goal.\n",
    "\n",
    "With maze in state it's specially important to pick a good heuristic - after all, the search space is exponentially large, as each subset of eaten food represents a different node even with the agent in a fixed position. Considering that in this case we have information that allows a more realistic approach, but keeping it simple in terms of code, we implement the sum of Manhattan distances between the agent and all the foods as a heuristic, as it's highly possible that most of them will be eaten in the optimal path. Notice that this sum can overestimate the optimal, because it's not always true that all the foods will be eaten in the best path. As a overestimating heuristic, it breaks admissibility - that is, A\\* is not guaranteed to find the optimal path. Even so, as our problem gives a high score to Pac-Man when it eats, we chose it expecting A\\* will find good paths in reasonable running times. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global reference to goal => problem 1 heuristic needs it\n",
    "# TODO - Avoid this (hardcode)\n",
    "goal_ref = None\n",
    "\n",
    "# Heuristic for problem 1 - without maze in state\n",
    "def astar_heuristic_p1(node):\n",
    "    ''' manhattan distance between Pac-Man and goal '''\n",
    "    \n",
    "    idx = node.state\n",
    "    md = manhattan_distance(goal_ref, idx)\n",
    "    return md\n",
    "\n",
    "# Heuristic for problem 2 - with maze in state\n",
    "def astar_heuristic(node):\n",
    "    ''' sum of manhattan distances between Pac-Man and all foods in maze '''\n",
    "    \n",
    "    # Detach maze configuration and Pac-Man position\n",
    "    tuple_maze, idx = node.state\n",
    "    \n",
    "    # Accumulate sum of manhattan distances to foods\n",
    "    md_sum = 0\n",
    "    for food_idx in np.argwhere(maze == '.'):\n",
    "        md_sum += manhattan_distance(food_idx, idx)\n",
    "            \n",
    "    return md_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **? Search Solution**\n",
    "###### **Responsible:** Matheus\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction\n",
    "- [ ] Heuristics choosen for each problem variation\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Local Search Methods**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simulated Annealing Solution**\n",
    "###### **Responsible:** Vinicius\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction\n",
    "- [ ] Heuristics choosen for each problem variation\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparisions**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Compare methods and problems\n",
    "- [ ] Maybe display a graph comparing scores\n",
    "- [ ] Maybe animate one maze with all methods running (different colors to distinguish agents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('mc906': virtualenv)",
   "language": "python",
   "name": "python36964bitmc906virtualenv7dec5718c7b3422688b8879a7762f26b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

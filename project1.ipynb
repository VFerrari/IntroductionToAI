{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 1 - Search-based solutions for static Pac-Man game**\n",
    "**Subject:** MC906/MO416 - Introduction to Artificial Intelligence \n",
    "\n",
    "**Authors:**\n",
    "\n",
    "    Daniel Helú Prestes de Oliveira - RA 166215\n",
    "    Eduardo Barros Innarelli        - RA 170161\n",
    "    Matheus Rotta Alves             - RA 184403\n",
    "    Victor Ferreira Ferrari         - RA 187890\n",
    "    Vinícius Couto Espindola        - RA 188115\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Problem description \n",
    "- [ ] Problem modeling\n",
    "- [ ] Search agent (motivation, API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Description**\n",
    "Consider the Pacman video game. We're given a maze where each position can have one of the following: start (unique), goal (unique), enemy, dots, walls or empty spaces. The pacman will begin the game in the start position and will try to reached the goal position only crossing through empty spaces and dots positions. In the event were pacman reaches outside the map, it's position is \"wrapped around\" the map, meaning that it will appear on the oposite side of the map maintaining one of it's coodinates.\n",
    "When compared to the original game, some simplifications were made: \n",
    " - Ghosts do not move during the pacmans search (their initial position is their only position) \n",
    " - Berries were removed from the game, therefore Pacman has no countermeasure against ghosts.\n",
    "\n",
    "### **Modeling**\n",
    " For every move pacman makes, it will pay a cost of 1 point, however, if a dot is found, it will receive a payment of 10 points. The sum of all costs and payments will be considered the score achieve by Pacman when executing a certain path. Also, Pacman can only move one position at a time. \n",
    "Based on such maze and restrictions, we must seek the best feasible path for Pacman to reach the goal. In our case, \"best\" refers to the path in which we achieve the highest score.\n",
    "\n",
    "**Environment:** Is the maze described by the problem.\n",
    "\n",
    "**Actuator:** The only action allowed to pacman is to move to a neighbor cel in the maze given the problems restrictions.\n",
    "\n",
    "**Sensors:** We consider that pacman can \"see\" the entirety of the given maze.\n",
    "\n",
    "**Known:** The consquences to the environment given an action are completely predictable.\n",
    "\n",
    "**Environment Properties:**\n",
    " - **Fully observable:** We can see the entire maze.\n",
    " - **Deterministic:** The next state is entirely defined by the current state an a possible action on it.\n",
    " - **Static:** The only change that occurs within the maze is cause by Pacman himself, the ghosts do not act.\n",
    " - **Single Agent:** The only agent within the problem is Pacman.\n",
    " - **Observability:** ??\n",
    " - **Sequential:** The state of the environment is completely predictable and is only affected by Pacman himself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test Cases**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Maybe show two examples (one dense and it's correspondent sparse)\n",
    "\n",
    "For testing purposes, we generated 10 mazes using the [tool provided by classmate Gabriel Bomfim](https://gabomfim.github.io/pacman-mazegen/tetris/many.htm) in Google Classrom, which adapts the [maze generator](https://shaunlebron.github.io/pacman-mazegen/) linked in the project description. Each tile is represented by a char, where **|** and **-** are walls, **.** are foods and **o** are ghosts. For each maze, we choosed three start and goal positions, respectively symbolized by **!** and **?**.\n",
    "\n",
    "As this tool creates mazes fully filled with food, we thought that it would be good for comparision to also test sparse mazes, which we created by randomly removing dots in the dense ones. These variations, together with the originals, give us a total of 60 mazes, stored in `./mazes` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mazes.get_mazes import get_mazes\n",
    "denses, sparses = get_mazes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Uninformed Search Methods**\n",
    "\n",
    "> TODO List:\n",
    "- [X] Short theoretical introduction\n",
    "\n",
    "The Uninformed Search Methods, also known as Blind Search Methods, are algorithms that are given no information about the problem other than its definition. They are only able to generate possible successor of a state and analyze this sucessors in a sequence according to the algorithm applied. They analyze each successor created until it finds the one state that is equal to the goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Breadth-First Search Solution**\n",
    "###### **Responsible:** Victor\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table \n",
    "- [ ] Analysis with relevant(s) animation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Depth-First Search Solution**\n",
    "###### **Responsible:** Daniel\n",
    "\n",
    "> TODO List:\n",
    "- [X] Short theoretical introduction\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table \n",
    "- [ ] Analysis with relevant(s) animation(s)\n",
    "\n",
    "The Depth-First Search is a Uninformed Search method that expands its deepest node possible first, the one that has no sucessor. If the node is the goal state, it has found the path to the solution, otherwise it goes back to the most recent ancestor node that still has successor that were not expanded and it expands the deepest sucessor this ancestor node has.\n",
    "\n",
    "For the Pacman Problem the goal state considered for the problem was the final position that pacman should go. It did not consider analyzing all the possible outcomes of eating the dots to increase it points because the time and memory to consider all possible states would be exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Informed Search Methods**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A* Search Solution**\n",
    "###### **Responsible:** Eduardo\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)\n",
    "\n",
    "As an informed search algorithm, A\\* takes into account information about the path cost together with an heuristic to evaluate which is the most promising path to take when it enters a state. For this evaluation, A\\* chooses in state $n$ to proceed to the neighbor that gives the lowest $f(n) = g(n) + h(n)$, $g(n)$ being the exact path cost from starting state to $n$ and $h(n)$ the heuristic estimated cost from $n$ to goal state.\n",
    "\n",
    "#### **Heuristic**\n",
    "\n",
    "How fast the agent reaches the goal in A\\* depends on the heuristic implemented and how it affects the nodes expansion. Without the maze in state this isn't that much of a concern, as for a $n\\times m$ maze there can be up to $O(nm)$ states. It's pretty hard, though, to estimate a good cost to goal without the current configuration knowledge (the combinations of foods can make any estimation over the initial maze very far from optimal). To simplify, we use the **Manhattan distance** as a heuristic for this variation of the problem, which is the distance between the agent and the goal positions measured along axes at right angles (i.e., $|x_1 - x_2| + |y_1 - y_2|$, given that the agent is in $(x_1, y_1)$ and the goal is to reach $(x_2, y_2)$). It's a admissible heuristic as there can't be a shorten path from node to goal.\n",
    "\n",
    "With maze in state it's specially important to pick a good heuristic - after all, the search space is exponentially large, as each subset of eaten food represents a different node even with the agent in a fixed position. Considering that in this case we have information that allows a more realistic approach, but keeping it simple in terms of code, we implement the sum of Manhattan distances between the agent and all the foods as a heuristic, as it's highly possible that most of them will be eaten in the optimal path. Notice that this sum can overestimate the optimal, because it's not always true that all the foods will be eaten in the best path. As a overestimating heuristic, it breaks admissibility - that is, A\\* is not guaranteed to find the optimal path. Even so, as our problem gives a high score to Pac-Man when it eats, we chose it expecting A\\* will find good paths in reasonable running times. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global reference to goal => problem 1 heuristic needs it\n",
    "# TODO - Avoid this (hardcode)\n",
    "goal_ref = None\n",
    "\n",
    "# Heuristic for problem 1 - without maze in state\n",
    "def astar_heuristic_p1(node):\n",
    "    ''' manhattan distance between Pac-Man and goal '''\n",
    "    \n",
    "    idx = node.state\n",
    "    md = manhattan_distance(goal_ref, idx)\n",
    "    return md\n",
    "\n",
    "# Heuristic for problem 2 - with maze in state\n",
    "def astar_heuristic_p2(node):\n",
    "    ''' sum of manhattan distances between Pac-Man and all foods in maze '''\n",
    "    \n",
    "    # Detach maze configuration and Pac-Man position\n",
    "    tuple_maze, idx = node.state\n",
    "    \n",
    "    # Accumulate sum of manhattan distances to foods\n",
    "    md_sum = 0\n",
    "    for food_idx in np.argwhere(maze == '.'):\n",
    "        md_sum += manhattan_distance(food_idx, idx)\n",
    "            \n",
    "    return md_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **? Search Solution**\n",
    "###### **Responsible:** Matheus\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction\n",
    "- [ ] Heuristics choosen for each problem variation\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Local Search Methods**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simulated Annealing Solution**\n",
    "###### **Responsible:** Vinicius\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Short theoretical introduction\n",
    "- [ ] Heuristics choosen for each problem variation\n",
    "- [ ] Run tests script with and without maze in state\n",
    "- [ ] Results table\n",
    "- [ ] Analysis with relevant(s) animation(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparisions**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Compare methods and problems\n",
    "- [ ] Maybe display a graph comparing scores\n",
    "- [ ] Maybe animate one maze with all methods running (different colors to distinguish agents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

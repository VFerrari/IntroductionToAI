{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook dependencies\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 1 - Search-based solutions for static Pac-Man game**\n",
    "**Subject:** MC906/MO416 - Introduction to Artificial Intelligence \n",
    "\n",
    "**Authors:**\n",
    "\n",
    "    Daniel Helú Prestes de Oliveira - RA 166215\n",
    "    Eduardo Barros Innarelli        - RA 170161\n",
    "    Victor Ferreira Ferrari         - RA 187890\n",
    "    Vinícius Couto Espindola        - RA 188115\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "### **Project**\n",
    "  The project consists of a study of different search methods in goal-based solutions to a variation of the **Pac-Man** game. Five search methods were tested and will be discussed here, being:\n",
    "- Two **uninformed** search methods (Breadth-First Search, Depth-First Search);\n",
    "- Two **informed** search methods ($A^*$ Search, Greedy Best-First Search);\n",
    "- One **local search** method (Simulated Annealing).\n",
    "\n",
    "  Aside from the search methods, the modeling of the problem will also be discussed and tested, with characteristics such as:\n",
    "- State representations (multiple);\n",
    "- Set of actions;\n",
    "- Objective state test;\n",
    "- The path cost.\n",
    "\n",
    "The models with different methods will be tested in different environments, and discussed accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Usage**\n",
    "This project uses the _AIMA_ Python library for a generic implementation of the different search methods. You (the reader) can use any way you want to include that library in this notebook, but the following cell adds the library to the Python path if it is in the same folder as this notebook (if you clone the GitHub repository and its submodules, it should already be there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = os.path.abspath('')\n",
    "sys.path.insert(0,f'{direct}/aima-python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other external libraries used by the project are _NumPy_ for better array manipulation (maze), _Pandas_ for data organization in testing, _PyGame_ for visualization/animation of solutions and _wrapt_timeout_decorator_ for timeout in time-intensive methods. The other imports are part of the Standard Python Library.\n",
    "\n",
    "The timeout decorator library and all other external libraries can be installed via _pip_. **The timeout decorator may not work as intended in Windows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygame as py\n",
    "from wrapt_timeout_decorator import timeout\n",
    "from search import breadth_first_graph_search, depth_first_graph_search, astar_search, greedy_best_first_graph_search, simulated_annealing_full\n",
    "from search import manhattan_distance, euclidean_distance, memoize, Problem\n",
    "\n",
    "# Python libraries\n",
    "import re, time\n",
    "from itertools import product as combine\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "np.set_printoptions(linewidth=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Problem**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Motivation and General Information**\n",
    "Consider the Pac-Man video game. The problem at hand is a variation of the game. We're given a maze where each position can have one of the following: start (unique), goal (unique), enemy, dots, walls or empty spaces. The Pac-Man will begin the game in the start position and will try to reached the goal by only crossing through empty spaces and dots positions. In the event were Pac-Man reaches outside the map, it's position is \"wrapped around\" the map, meaning that it will appear on the oposite side of the map maintaining one of it's coodinates.\n",
    "When compared to the original game, some simplifications were made: \n",
    " - Ghosts do not move during the search (their initial position is their only position). \n",
    " - Berries were removed from the game, therefore Pac-Man has no countermeasure against ghosts.\n",
    "\n",
    "### **Details and Environment**\n",
    "**Environment:** Is the maze described by the problem.\n",
    "\n",
    "**Actuator:** The only action Pac-Man is allowed to execute is moving to a neighbor cell in the maze given the problems restrictions.\n",
    "\n",
    "**Sensors:** We consider that Pac-Man can \"see\" the entirety of the given maze.\n",
    "\n",
    "**Known:** The consquences to the environment given an action are completely predictable.\n",
    "\n",
    "**Environment Properties:**\n",
    " - **Fully Observable:** We can see the entire maze at all times.\n",
    " - **Deterministic:** The next state is entirely defined by the current state and a possible action on it.\n",
    " - **Static:** The only change that occurs within the maze is caused by Pac-Man himself, the ghosts do not act.\n",
    " - **Discrete:** The environment has a finite number of states, being a finite maze with finite resources.\n",
    " - **Single Agent:** The only agent within the problem is Pac-Man.\n",
    " - **Sequential:** Every step builds on the previous ones to traverse the state space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Modeling Decisions**\n",
    "  For every move Pac-Man takes, it will pay a **cost of 1 point**. However, if a dot is eaten, it will receive a **payment of 10 points**. The sum of all costs and payments will be considered the score achieved by Pac-Man when executing a certain path. The path cost is the opposite, so the relation is $score = -1 . pathcost$. Pac-Man can only move one position at a time, in the down, up, left and right directions. The nodes are expanded in that exact order. The goal has the same score/path cost than a dot, not being worth more for the score.\n",
    "  \n",
    "Based on the maze and general problem restrictions, we can seek the best feasible path for Pac-Man to reach the goal. In our case, \"best\" refers to the path in which we achieve the highest score. However, since goal-based agents are being used, the **primary goal** is to reach the final position. This distinction is important for some methods.\n",
    "\n",
    "To save RAM, the maze is stored as **bytes** (ASCII characters), instead of unicode chars.\n",
    "\n",
    "### **Different Models**\n",
    "  For such a problem, there are many models that could be made to solve it, with potentially different state spaces, solutions and solution quality levels for each. Some could also be more appropriate for some methods than others. For that reason, it was decided that not one model would be the best for every situation, and the quality of the analysis would be better with **two** models being used with **every** method. The chosen models directly change the state space, and indirectly change what is the priority in achieving the goal.\n",
    "  \n",
    "  The first model has each state being a tuple representing **only** the position of Pac-Man in the maze. So, each position of the maze can only be visited **once** in graph-related searches (which almost all methods used are based on). This results in a **quadratic** state space, with $O(nm)$ states for a $n\\times  m$ maze, which is good for the most high-complexity methods. This model also focuses more on **reaching the goal** than maximizing the score, since it does not consider paths that use the same position twice, in favor of eating more dots. This could waste a lot of potentially better results in the informed methods. The position control is made using a \"global\" maze as a class atribute in the problem.\n",
    "  \n",
    "  The second model has each state being a tuple with both the position of Pac-Man in the maze **and the current maze itself**. This model accepts the possiblity of visiting the same position in the maze twice **only if** the maze is different (a dot has been eaten somewhere since the last time). The state space is still finite, but **exponential**, so this could lead to very high memory usage and execution time in some methods. This model also lets the method decide the priority: maximizing the score or reaching the goal. Since the state includes the maze configuration, it can be used to try to maximize the score before heading to the goal by going through possibly the same position more than once. The maze in the state is stored as a **tuple of tuples**, since a state should be hashable and tuples are immutable.\n",
    "  \n",
    "  Other models were considered with less information about the maze inside a state, but for the scope of this project these two models were the ones used. In the \"Conclusions\" section, other alternatives are suggested for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem Model 1: State With Only the Coordintes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 30: PacProblemNoMaze.py\n",
    "class Problem1(Problem):\n",
    "    ''' Modeling the static Pac-Man game problem for search. '''\n",
    "    \n",
    "    def __init__(self, initial, goal, maze, heuristic = None):\n",
    "        ''' Initial State:\n",
    "            Tuple of 2 elements. (i,j) in maze.\n",
    "            Goal State:\n",
    "            Tuple of 2 elements. (i,j) in maze.\n",
    "            Maze:\n",
    "            NumPy array of BYTES (to save RAM)\n",
    "        '''\n",
    "        Problem.__init__(self, initial, goal)\n",
    "        self.maze = maze\n",
    "        self.heuristic = heuristic\n",
    "        self.visited = set()\n",
    "        self.explored = set()\n",
    "        self.repeated_states = 0\n",
    "        self.current = [None,0]\n",
    "        \n",
    "    def actions(self, state):\n",
    "        ''' A state is the index of the maze (tuple). \n",
    "            An action is a tuple of i,j with the direction to walk.\n",
    "        '''\n",
    "        # Register new visited nodes\n",
    "        hashing = state[0]*100+state[1]\n",
    "        if hashing in self.visited: self.repeated_states += 1\n",
    "        self.visited = self.visited.union([hashing])\n",
    "        \n",
    "        # Keep state and it's score for heuristic analisys\n",
    "        self.current[0] = state\n",
    "        self.current[1] += 1\n",
    "\n",
    "        actions = []\n",
    "        possible = [(1,0),(-1,0),(0,1),(0,-1)]\n",
    "        idx = state\n",
    "\n",
    "        # This is the new behavior of eating the points\n",
    "        # Eat point if needed\n",
    "        if self.maze[idx] == b'.':\n",
    "            self.current[1] -= 9\n",
    "            self.maze[idx] = b' '\n",
    "        for action in possible:\n",
    "            nxt = list(map(sum, zip(idx,action)))\n",
    "            \n",
    "            # Check circling around maze. If < 0, negative indexing will do the job.\n",
    "            if nxt[0] == self.maze.shape[0]:\n",
    "                nxt[0] = 0\n",
    "            elif nxt[1] == self.maze.shape[1]:\n",
    "                nxt[1] = 0\n",
    "            nxt = tuple(nxt)\n",
    "            \n",
    "            # Check ghosts and walls.\n",
    "            if self.maze[nxt] not in [b'o', b'|', b'-']:\n",
    "                actions.append(action)\n",
    "        \n",
    "        # Register new explored nodes\n",
    "        hashing = [(i+idx[0])*100 + (j+idx[1]) for i,j in actions]\n",
    "        self.explored = self.explored.union(hashing)\n",
    "\n",
    "        return actions\n",
    "\n",
    "    def goal_test(self, state):\n",
    "        ''' Check if the Pac-Man reaches its destination.'''\n",
    "        return state == self.goal\n",
    "\n",
    "    def result(self, state, action):\n",
    "        ''' The result of an action is to move to the next position, and eat the point if needed.'''\n",
    "        idx = state\n",
    "        \n",
    "        # Get next position.\n",
    "        nxt = list(map(sum, zip(idx,action)))\n",
    "        \n",
    "        # Circle around maze.\n",
    "        if nxt[0] == self.maze.shape[0]:\n",
    "            nxt[0] = 0\n",
    "        elif nxt[0] < 0:\n",
    "            nxt[0] = self.maze.shape[0]-1\n",
    "        elif nxt[1] == self.maze.shape[1]:\n",
    "            nxt[1] = 0\n",
    "        elif nxt[1] < 0:\n",
    "            nxt[1] = self.maze.shape[1]-1\n",
    "        \n",
    "        nxt = tuple(nxt)\n",
    "\n",
    "        return nxt\n",
    "    \n",
    "    def path_cost(self, c, state1, action, state2):\n",
    "        ''' 10 points if it eats a point, and minus 1 point per movement. '''\n",
    "        nxt = self.maze[state2]\n",
    "        # Goal is same as a point (for now)\n",
    "        if nxt == b'.' or nxt == b'?':\n",
    "            cost = c-10\n",
    "        else:\n",
    "            cost = c\n",
    "        return cost+1\n",
    "\n",
    "    def value(self, state):\n",
    "        ''' Value is the \"score\" for the state.'''\n",
    "        # Use euclidean distance as a heuristic\n",
    "        if self.heuristic == 'euclidean':\n",
    "            return -5*euclidean_distance(state, self.goal)\n",
    "\n",
    "        # Use manhatam distance as a heuristic\n",
    "        if self.heuristic == 'manhattan':\n",
    "            return -5*manhattan_distance(state, self.goal)\n",
    "\n",
    "        # Use manhatam sum value as a heuristic\n",
    "        if self.heuristic == 'pathcost':\n",
    "            if state == self.current[0]:\n",
    "                cost = self.current[1]\n",
    "            elif self.maze[state] == b'.':\n",
    "                cost = self.current[1] - 9\n",
    "            else:\n",
    "                cost = self.current[1] + 1\n",
    "            return -1*cost\n",
    "\n",
    "        # Error if no heuristic defined\n",
    "        if not self.heuristic:\n",
    "            raise Exception(\"CHOOSE A HEURISTIC before executing\")\n",
    "        if callable(self.heuristic):\n",
    "            raise Exception(\"CHOOSE A HEURISTIC NAME before executing, not a function.\")\n",
    "        \n",
    "    def h(self, node):\n",
    "        ''' Heuristic for informed/local search methods '''\n",
    "        \n",
    "        assert self.heuristic != None, \"Heuristic must be set!\"\n",
    "        assert callable(self.heuristic), \"Heuristic must be a function!\"\n",
    "        \n",
    "        # Need to receive maze to estimate\n",
    "        return self.heuristic(node, self.maze)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem Model 2: State With Coordinates and Maze**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# %load -r 29: PacProblem.py\n",
    "class Problem2(Problem):\n",
    "    ''' Modeling the static Pac-Man game problem for search. '''\n",
    "    \n",
    "    def __init__(self, initial, goal, heuristic = None):\n",
    "        ''' Initial State:\n",
    "            Tuple of 2 elements. 1-Initial maze. 2. (i,j) in maze.\n",
    "            Goal State:\n",
    "            Tuple of 2 elements. (i,j) in maze.\n",
    "        '''\n",
    "        Problem.__init__(self, initial, goal)\n",
    "        self.visited = set()\n",
    "        self.explored = set()\n",
    "        self.repeated_states = 0\n",
    "        self.counter = 0\n",
    "        self.heuristic = heuristic\n",
    "        \n",
    "    def actions(self, state):\n",
    "        ''' \n",
    "            A state is the current maze (tuple of tuples) and the agent index\n",
    "            in the maze (tuple). An action is a tuple of i,j with the direction \n",
    "            to walk.\n",
    "        '''\n",
    "        self.visited = self.visited.union([self.counter])\n",
    "\n",
    "        actions = []\n",
    "        possible = [(1,0),(-1,0),(0,1),(0,-1)]\n",
    "        tuple_maze, idx = state\n",
    "                \n",
    "        # Convert maze into a numpy array.\n",
    "        maze = np.array(tuple_maze)\n",
    "        \n",
    "        for action in possible:\n",
    "            nxt = list(map(sum, zip(idx,action)))\n",
    "                        \n",
    "            # Check circling around maze. If < 0, negative indexing will do the job.\n",
    "            if nxt[0] == maze.shape[0]:\n",
    "                nxt[0] = 0\n",
    "            elif nxt[0] < 0:\n",
    "                nxt[0] = maze.shape[0]-1\n",
    "            elif nxt[1] == maze.shape[1]:\n",
    "                nxt[1] = 0\n",
    "            elif nxt[1] < 0:\n",
    "                nxt[1] = maze.shape[1] - 1\n",
    "                \n",
    "            nxt = tuple(nxt)\n",
    "            \n",
    "            # Check ghosts and walls.\n",
    "            if maze[nxt] not in [b'o', b'|', b'-']:\n",
    "                actions.append(action)\n",
    "        \n",
    "        self.explored = self.explored.union(range(self.counter,self.counter+4))\n",
    "        self.counter += 4\n",
    "        return actions\n",
    "\n",
    "    def goal_test(self, state):\n",
    "        ''' Check if the Pac-Man reaches its destination.'''\n",
    "        return state[1] == self.goal\n",
    "\n",
    "    def result(self, state, action):\n",
    "        ''' The result of an action is to move to the next position, and eat the point if needed.'''\n",
    "        tuple_maze, idx = state\n",
    "        \n",
    "        # Convert maze into a numpy array.         \n",
    "        maze = np.array(tuple_maze)\n",
    "        \n",
    "        # Get next position.\n",
    "        nxt = list(map(sum, zip(idx,action)))\n",
    "        \n",
    "        # Circle around maze.\n",
    "        if nxt[0] == maze.shape[0]:\n",
    "            nxt[0] = 0\n",
    "        elif nxt[0] < 0:\n",
    "            nxt[0] = maze.shape[0]-1\n",
    "        elif nxt[1] == maze.shape[1]:\n",
    "            nxt[1] = 0\n",
    "        elif nxt[1] < 0:\n",
    "            nxt[1] = maze.shape[1]-1\n",
    "        \n",
    "        nxt = tuple(nxt)\n",
    "        \n",
    "        # Eat point if needed\n",
    "        if maze[nxt] == b'.':\n",
    "            maze[nxt] = b' '\n",
    "        return tuple(map(tuple, maze)), nxt\n",
    "    \n",
    "    def path_cost(self, c, state1, action, state2):\n",
    "        ''' 10 points if it eats a point, and minus 1 point per movement. '''\n",
    "        nxt = np.array(state1[0])[state2[1]]\n",
    "        \n",
    "        # Goal is same as a point (for now)\n",
    "        if nxt == b'.' or nxt == b'?':\n",
    "            cost = c-10\n",
    "        else:\n",
    "            cost = c\n",
    "        return cost+1\n",
    "\n",
    "    def value(self, state):\n",
    "        ''' Value is the \"score\" for the state.'''\n",
    "        return -1 * self.path_cost(None, None, None, state)\n",
    "    \n",
    "    def h(self, node):\n",
    "        ''' Heuristic for informed/local search methods '''\n",
    "        \n",
    "        assert self.heuristic != None, \"Heuristic must be set!\"\n",
    "        \n",
    "        # No need to receive maze to estimate, as it is stored in node.state\n",
    "        return self.heuristic(node)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Solution Visualization**\n",
    "In order to create a nice way to visualize each search behaviours, we used PyGame library visual interface to draw the maze with the Pac-Man's path. This was achieved by transforming the matrix with the maze in a matrix which mapped the display pixels. For the creation and manipulation of such, we defined a class PacScreen which serves as an interface between the search agent and the screen.\n",
    "\n",
    "To create the pixels map we simply represent each position of the matrix with a offset defined as `px=28` in PacScreen. This allows us to have a `px` square block to draw the contents of each position. To reference the block, we simply use the matrix indexes of the position we want to draw and multiply then by `px`, this gives us the top-left vertex of the corresponding block. However, since matrices are indexed as `M[i,j]`, where `i` is the line and `j` the column, and the display is indexed with cartesian coordinates `(x,y)`, we must map the indexes `i` and `j` to `y` and `x`, respectively.\n",
    "\n",
    "The PacScreen class works through the following steps: \n",
    " * First we initialize the class by setting essential parameters such as: a copy of the maze to be used as reference, the size of the screen (considering how many pixels each block will hold), the Pacman's initial position and the goal position.\n",
    " * Once we've ran a search with the same maze in PacScreen, we create a sequence of indexes `(i,j)` which indicate the Pac-Man's path in the maze. This sequence is then passed to the `PacScreen.run()` function were the maze is drawn in each of the positions visited by Pac-Man.\n",
    " * Inside the `run()` function, we fist draw the starting environment with `PacScreen.draw()`. Then, every `interval=0.005` seconds we update the display with the next position using `PacScreen.update()`. The `update()` function is a light weight version of `draw()` where we only update what is necessary.\n",
    " \n",
    "**Note:** PyGame provides funcions which allows us to draw circles, rectangle and polygons, these were used to represent the map contained in the matrix. The walls are blocks filled with blue rectangles, the ghosts are triangles, the goal is a red circle, the dots are white circles and Pacman is represented by the yellow circle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='images/display_transform.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 26:125 PacScreen.py\n",
    "\n",
    "black = (0,0,0)\n",
    "grey = (50,50,50)\n",
    "white = (255,255,255)\n",
    "red = (255,0,0)\n",
    "green = (0,255,0)\n",
    "blue = (0,0,200)\n",
    "yellow = (255,255,0)\n",
    "\n",
    "# Múltiple of 4 ples\n",
    "px = 28\n",
    "\n",
    "class PacScreen():\n",
    "\n",
    "    def __init__(self, maze):\n",
    "        self.maze = maze.copy()\n",
    "        self.size = (self.maze.shape[1]*px,self.maze.shape[0]*px)\n",
    "\n",
    "        self.pac = tuple(map(int, np.where(self.maze==b'!')))\n",
    "        self.goal = tuple(map(int, np.where(self.maze==b'?'))) \n",
    "\n",
    "    def draw(self, maze, pac, goal):\n",
    "        walls = (maze==b'|')\n",
    "        bars = (maze==b'-')\n",
    "        dots = (maze==b'.')\n",
    "        ghosts = (maze==b'o')\n",
    "        y_max,x_max = maze.shape\n",
    "\n",
    "        for i in range(y_max):\n",
    "            for j in range(x_max):\n",
    "                x = j*px\n",
    "                y = i*px\n",
    "                if walls[i][j]:\n",
    "                    py.draw.rect(self.disp, blue, (x,y,px,px))\n",
    "                if bars[i][j]:\n",
    "                    py.draw.rect(self.disp, white, (x,int(y+px/2-px/4),px,int(px/4)))\n",
    "                if dots[i][j]:\n",
    "                    x_c = int(x+px/2)\n",
    "                    y_c = int(y+px/2)\n",
    "                    rad = int(px/5)\n",
    "                    py.draw.circle(self.disp, white, (x_c,y_c), rad)\n",
    "                if ghosts[i][j]:\n",
    "                    a = (x,y+px)\n",
    "                    b = (x+px,y+px)\n",
    "                    c = (int(x+px/2),y)\n",
    "                    py.draw.polygon(self.disp, green, (a,b,c))\n",
    "        \n",
    "        x_c = int(pac[1]*px + px/2)\n",
    "        y_c = int(pac[0]*px + px/2)\n",
    "        rad = int(px/3)\n",
    "        py.draw.circle(self.disp, yellow, (x_c,y_c), rad)\n",
    "\n",
    "        x_c = int(goal[1]*px + px/2)\n",
    "        y_c = int(goal[0]*px + px/2)\n",
    "        rad = int(px/4)\n",
    "        py.draw.circle(self.disp, red, (x_c,y_c), rad)\n",
    "\n",
    "    def update(self, maze, pac):\n",
    "        # Erase old pacman\n",
    "        x = self.pac[1]*px\n",
    "        y = self.pac[0]*px\n",
    "        py.draw.rect(self.disp, black, (x,y,px,px))\n",
    "        \n",
    "        # Leave visited tag\n",
    "        x_c = int(self.pac[1]*px + px/2)\n",
    "        y_c = int(self.pac[0]*px + px/2)\n",
    "        rad = int(px/4)\n",
    "        py.draw.circle(self.disp, grey, (x_c,y_c), rad)\n",
    "\n",
    "        # Update and draw\n",
    "        self.pac = pac\n",
    "        x_c = int(self.pac[1]*px + px/2)\n",
    "        y_c = int(self.pac[0]*px + px/2)\n",
    "        rad = int(px/3)\n",
    "        py.draw.circle(self.disp, yellow, (x_c,y_c), rad)\n",
    "\n",
    "    def step(self, action):    \n",
    "        if self.maze[action] == b'.':\n",
    "            self.maze[action] = ' ' \n",
    "        self.update(self.maze, action)\n",
    "\n",
    "    def run(self, path, interval=0.005):\n",
    "        # Create window and draw\n",
    "        self.disp = py.display.set_mode(self.size)\n",
    "        self.disp.fill(black)\n",
    "        self.map = py.PixelArray(self.disp)\n",
    "        self.draw(self.maze, self.pac, self.goal)\n",
    "\n",
    "        # Animate\n",
    "        while True:\n",
    "            for event in py.event.get():\n",
    "                if event.type == py.QUIT:\n",
    "                    py.quit()\n",
    "                    return\n",
    "\n",
    "            if path:\n",
    "                self.step(path.pop(0))\n",
    "                py.display.update()\n",
    "            time.sleep(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Search Agent**\n",
    "\n",
    "### **Motivation**\n",
    "This project consists of different search solutions for the same few problem models, and most methods use the same data structures. In that sense, the logical step to take is to create a shared API for every method, so that the environment is the same for every method, standardizing testing and result analysis. This agent should be able to formulate any of the problem models, change problem variables/properties and provide an API for result analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **API**\n",
    "The SearchAgent class can be seen in the cell bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 26: SearchAgent.py\n",
    "class SearchAgent:\n",
    "    def __init__(self, maze):\n",
    "        self.maze = maze\n",
    "        self.display = None\n",
    "        self.problem = None\n",
    "        self.solution = None\n",
    "        self.init = None\n",
    "        self.state_maze = False\n",
    "        self.heuristic = None\n",
    "    \n",
    "    def set_maze(self, maze):\n",
    "        self.maze = maze\n",
    "        if self.problem and not self.state_maze:\n",
    "            self.problem.maze = maze.copy()\n",
    "            \n",
    "    def set_heuristic(self, heuristic):\n",
    "        self.heuristic = heuristic\n",
    "        if self.problem:\n",
    "            self.problem.heuristic = heuristic\n",
    "\n",
    "    def find_positions(self):\n",
    "        ''' Find initial and goal positions in correctly made mazes.'''\n",
    "        init = np.where(self.maze == b'!')\n",
    "        goal = np.where(self.maze == b'?')\n",
    "\n",
    "        # If init is not defined.\n",
    "        if init[0].size == 0:\n",
    "            init = None\n",
    "        else:\n",
    "            init = init[0][0], init[1][0]\n",
    "        \n",
    "        # If goal is not defined.\n",
    "        if goal[0].size == 0:\n",
    "            goal = None\n",
    "        else:\n",
    "            goal = goal[0][0], goal[1][0]\n",
    "        return init, goal\n",
    "    \n",
    "    def formulate_problem(self, initial_pos, goal_pos, with_maze, goal_conditions):\n",
    "        ''' Formulates problem based on positions and if maze is in state or not.'''\n",
    "        self.init = initial_pos        \n",
    "        \n",
    "        # Conditions\n",
    "        assert all(self.maze[initial_pos] != t for t in goal_conditions), \"Initial position does not satisfy conditions!\"\n",
    "        assert all(self.maze[goal_pos] != t for t in goal_conditions), \"Goal does not satisfy conditions!\"\n",
    "    \n",
    "        if not with_maze:\n",
    "            # Problem with maze in state\n",
    "            self.problem = Problem1(initial_pos, goal_pos, self.maze.copy(), self.heuristic)\n",
    "        else:\n",
    "            # Problem with only Pac-Man position in state\n",
    "            initial_pos = (tuple(map(tuple, self.maze)), initial_pos)\n",
    "            self.problem = Problem2(initial_pos, goal_pos, self.heuristic)\n",
    "        \n",
    "        self.state_maze = with_maze\n",
    "    \n",
    "    def search(self, method, *args):\n",
    "        ''' Execute search (solve problem). '''\n",
    "        self.solution = method(self.problem, *args)\n",
    "    \n",
    "    def get_solution(self):\n",
    "        if isinstance(self.solution, tuple):\n",
    "            return self.solution\n",
    "        elif self.solution:\n",
    "            return self.solution.solution()\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    def get_path(self):\n",
    "        if self.solution:\n",
    "            return self.solution.path()\n",
    "        else:\n",
    "            return []\n",
    "            \n",
    "    def get_score(self):\n",
    "        if self.solution:\n",
    "            return -1*self.solution.path_cost\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_explored(self):\n",
    "        if self.problem:\n",
    "            return len(self.problem.explored)\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "    def get_visited(self):\n",
    "        if self.problem:\n",
    "            return len(self.problem.visited)\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "    def get_repeated(self):\n",
    "        if self.problem:\n",
    "            return self.problem.repeated_states\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "\n",
    "    def transform_path(self):\n",
    "        ''' Transforms a path of nodes to a path of positions. '''\n",
    "        pos = []\n",
    "        path = self.solution.path()\n",
    "        \n",
    "        # Transform to positions            \n",
    "        # If maze is in state\n",
    "        if self.state_maze:\n",
    "            for node in path:\n",
    "                pos.append(node.state[1])\n",
    "        else:\n",
    "            for node in path:\n",
    "                pos.append(node.state)\n",
    "        return pos\n",
    "    \n",
    "    def display_path(self, path, interval=0.005):\n",
    "        ''' Animate maze, to visualize found path. '''\n",
    "        self.display = PacScreen(self.maze)\n",
    "        self.display.run(path, interval)\n",
    "    \n",
    "    def apply_actions(self, actions):\n",
    "        ''' Apply actions to maze in ascii, to visualize found path. '''\n",
    "        direct_x = {1:b'>', -1:b'<'}\n",
    "        direct_y = {-1:b'^', 1:b'v'}\n",
    "        directions = {b'^', b'v', b'<', b'>'}\n",
    "        maze = self.maze.copy()\n",
    "        pos = self.init\n",
    "        \n",
    "        # Initial\n",
    "        maze[pos] = b'!'\n",
    "        for act in actions:\n",
    "            pos = list(map(sum, zip(pos,act)))\n",
    "            \n",
    "            # Circle around maze\n",
    "            if pos[0] == maze.shape[0]:\n",
    "                pos[0] = 0\n",
    "            elif pos[0] < 0:\n",
    "                pos[0] = maze.shape[0]-1\n",
    "            elif pos[1] == maze.shape[1]:\n",
    "                pos[1] = 0\n",
    "            elif pos[1] < 0:\n",
    "                pos[1] = maze.shape[1]-1\n",
    "            pos = tuple(pos)\n",
    "            \n",
    "            if maze[pos] in directions:\n",
    "                continue\n",
    "            \n",
    "            if act[0] != 0:\n",
    "                maze[pos] = direct_y[act[0]]\n",
    "            elif act[1] != 0:      \n",
    "                maze[pos] = direct_x[act[1]]\n",
    "        \n",
    "        # Goal\n",
    "        maze[pos] = b'?'\n",
    "        return maze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SearchAgent lets the user:\n",
    "- Find the initial and goal positions of a maze from a standard as seen in the next section;\n",
    "- Formulate any of the problem models, given an initial position, a goal position, goal conditions and flags indicating the model to use;\n",
    "- Change the maze without changing the problem;\n",
    "- Formulate a new problem with the same maze;\n",
    "- Use any search method from AIMA and store its return object;\n",
    "- Get the solution (sequence of actions or path);\n",
    "- Get the solution path;\n",
    "- Get the total ammount of node visited by the search;\n",
    "- Get the final score from the solution;\n",
    "- Visualize the solution in ASCII form, returning a NumPy array with the maze modified with the taken path;\n",
    "- Use an animation engine created in _pygame_ to animate the final path.\n",
    "\n",
    "Therefore, it accomplishes its objective of providing a generic environment to run tests and solve the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SearchAgent(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test Cases**\n",
    "\n",
    "> TODO List:\n",
    "- [ ] Maybe show two examples (one dense and it's correspondent sparse)\n",
    "\n",
    "For testing purposes, we generated 10 mazes using the [tool provided by classmate Gabriel Bomfim](https://gabomfim.github.io/pacman-mazegen/tetris/many.htm) in Google Classroom, which adapts the [maze generator](https://shaunlebron.github.io/pacman-mazegen/) linked in the project description. Each tile is represented by a char, where **|** and **-** are walls, **.** are foods and **o** are ghosts. For each maze, we choosed three start and goal positions, respectively symbolized by **!** and **?**.\n",
    "\n",
    "As this tool creates mazes filled with food, we thought that it would be good for comparision to also test sparse mazes, which we created by randomly removing dots in the dense ones. These variations, together with the originals, give us a total of 60 mazes, stored in `./mazes` directory.\n",
    "\n",
    "The tests were run with the SearchAgent class defined in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='images/test_sample.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 7:13 testing.py\n",
    "# Getting test files\n",
    "path = 'mazes/'\n",
    "sizes = ['dense/','sparse/']\n",
    "maze = ['1','2','3','4','5','6','7','8','9','10']\n",
    "pos = ['a','b','c']\n",
    "test_files = [path+s+i+l for (s,i,l) in list(combine(sizes,maze,pos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 14:120 testing.py\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + ' ' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()  # As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)\n",
    "\n",
    "def collect_data(data, out_path):\n",
    "    # Gather info by grouping dense/sparse and map\n",
    "    ids = data.groupby(['type','id'])\n",
    "    ids_means = ids.mean()\n",
    "    ids_means.name = 'ids_means'\n",
    "    ids_max = ids.max().drop(columns='class')\n",
    "    ids_max.name = 'ids_max'\n",
    "    ids_min = ids.min().drop(columns='class')\n",
    "    ids_min.name = 'ids_min'\n",
    "\n",
    "    # Gather info by grouping dense/sparse\n",
    "    types = data.groupby(['type'])\n",
    "    types_means = types.mean()\n",
    "    types_means.name = 'types_mean'\n",
    "    types_max = types.max().drop(columns='class')\n",
    "    types_max.name = 'types_max'\n",
    "    types_min = types.min().drop(columns='class')\n",
    "    types_min.name = 'types_min'\n",
    "\n",
    "    dataframes = [data,ids_means,ids_max,ids_min,types_means,types_max,types_min,]\n",
    "    if out_path:\n",
    "        for df in dataframes: df.to_csv(f'{out_path}/{df.name}.csv')\n",
    "                        \n",
    "    return dataframes\n",
    "    \n",
    "#### TESTING ROUTINE ####\n",
    "\n",
    "def run_tests(test_files, search, *args, repeat=1, out_path=''):\n",
    "    print(\"#### Starting New Test Routine ####\")\n",
    "    keys = ['type','id','class',f'time_{repeat}avg',f'cost_{repeat}avg',f'fails_{repeat}total','visited','repeated','explored']\n",
    "    data = pd.DataFrame(columns=keys)\n",
    "    data.name='all_data'\n",
    "    count,total = 0, repeat*len(test_files)\n",
    "    agent = None\n",
    "\n",
    "    for maze_file in test_files: \n",
    "        maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "        if not agent: agent = SearchAgent(maze) # Build agent if not yet created\n",
    "        \n",
    "        deltas = []\n",
    "        fails = []\n",
    "        costs = []\n",
    "        visits = []\n",
    "        repeated = []\n",
    "        explored = []\n",
    "\n",
    "        for i in range(1,repeat+1):\n",
    "            count += 1\n",
    "            progress(count, total, status=f\"{maze_file} X{i:4d}\")            \n",
    "            \n",
    "            # Set up new map\n",
    "            agent.set_maze(maze)                # Reset agent's maze\n",
    "            init,goal = agent.find_positions()  # Reset positions\n",
    "\n",
    "            # Run and time it\n",
    "            t0 = time.perf_counter()\n",
    "            try:\n",
    "                cost = search(agent, maze, init, goal, *args)\n",
    "            except TimeoutError:\n",
    "                cost = 0\n",
    "            finally:\n",
    "                tf = time.perf_counter()\n",
    "\n",
    "            # Data acumulators\n",
    "            deltas += [tf - t0]\n",
    "            fails += [0 if cost else 1]\n",
    "            costs += [0 if not cost else cost]\n",
    "            visits += [agent.get_visited()]\n",
    "            repeated += [agent.get_repeated()]\n",
    "            explored += [agent.get_explored()]\n",
    "\n",
    "        fails_ratio = sum(fails)/repeat\n",
    "        deltas_avg = sum(deltas)/repeat # Consider failures\n",
    "        if fails_ratio < 1:\n",
    "            cost_avg = sum(costs)/(repeat-sum(fails)) # Don't consider failures\n",
    "        else:\n",
    "            cost_avg = 0\n",
    "        visits_avg = sum(visits)/repeat\n",
    "        repeated_avg = sum(repeated)/repeat\n",
    "        explored_avg = sum(explored)/repeat\n",
    "\n",
    "        # Fetch info from file name\n",
    "        match = re.match(r'mazes/(\\w+)/(\\d+)(\\w)', maze_file)\n",
    "        maze_type = match.group(1)\n",
    "        maze_id = int(match.group(2))\n",
    "        maze_class = match.group(3)\n",
    "\n",
    "        # Add results to dataframe\n",
    "        values = [maze_type,maze_id,maze_class,deltas_avg,cost_avg,\n",
    "                  fails_ratio,visits_avg,repeated_avg,explored_avg]\n",
    "        data.loc[maze_file] = dict(zip(keys, values))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return collect_data(data, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Uninformed Search Methods**\n",
    "The Uninformed Search Methods, also known as Blind Search Methods, are algorithms that are given no information about the problem other than its definition. They are only able to generate possible successors of a state and analyze these sucessors in a sequence according to the algorithm applied. They analyze each successor created until it finds the goal state. Every state receives the same treatment, and every decision is local according to a method that can only use local information of the problem (if any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Breadth-First Search (BFS) Solution**\n",
    "###### **Responsible:** Victor\n",
    "\n",
    "> TODO List:\n",
    "- [X] Short theoretical introduction\n",
    "- [X] Run tests script with and without maze in state\n",
    "- [ ] Results table \n",
    "- [X] Analysis with relevant(s) animation(s)\n",
    "\n",
    "#### **The Method**\n",
    "Breadth-First Search (BFS) is an Uninformed Search method that visits every state in order of distance from the initial state. So this method expands every successor once it reaches a node, and then visits each one of them in a predetermined order in a queue (FIFO). In the _AIMA_ library, this is done in the order of actions given by the problem.\n",
    "\n",
    "BFS, for the Pac-Man problem, is a complete method, since the maze is finite and in the **graph** variation the method does not visit the same state twice. The graph variation is used because there can be loops in the state space, so a tree is not the best representation of it. This method always gets the shortest path to the goal position, but it is **not** an optimal method for the problem, because not all paths have the same cost, eating more dots will result in better solutions.\n",
    "\n",
    "This method is **exact and deterministic**, so it will **always** return the same solution to an instance. For that reason, there's no use in averaging results from different executions, and the method can be analyzed with only one execution.\n",
    "\n",
    "#### **Pre-Analysis**\n",
    "Since this method always expands the shallowest node, memory and time can be a big problem, so problems with less successors in each node and/or shallower trees can make a huge difference in these constraints. For that reason, between the two models created for the problem, the one that does not have the maze in its state is the likely best one for this method. \n",
    "\n",
    "Since BFS always finds the shortest path to the goal and always visits the shallowest node available, it will never need to check the same position twice. It will go through every position in the maze in order of proximity to the initial point, eventually getting to the goal without needing to cross a taken path. So the quadratic state space of the model with only the position as state will likely get the same answer as the exponential state space of the other model, but faster.\n",
    "\n",
    "And this increase in speed (and memory usage) can be pivotal, for this method can easily explode in time and memory with the more complex model. However, this likely behavior also hints that the result will not include a very high score, because the path cost is not considered at all in this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for Model Without Maze in State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_no_maze(agent, maze, init, goal, *args):\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.search(breadth_first_graph_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    bfs_no_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/bfs/problem1'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for Model With Maze in State**\n",
    "\n",
    "CAUTION: the below cell can take a really long time to run, and use a lot of RAM if timeout is too slow or not working. Timeout **does not work on Windows**, so if that's the used operating system, take out/comment that line, and be careful with RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeout(60, use_signals=True)\n",
    "def bfs_with_maze(agent, maze, init, goal, *args):\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.search(breadth_first_graph_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    bfs_with_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/bfs/problem2'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis**\n",
    "As discussed in the pre-analysis, the diference in time requirements between both models is very high, for the exact same answers. This is a specific characteristic of BFS, since it always takes the shortest route, and it never crosses the same position twice. It was not possible to run every maze with BFS in the second model, because the execution time \"explodes\", as does the memory usage, so the execution fails after a timeout.\n",
    "\n",
    "The Pac-Man scores result for this method are not consistent, since BFS does not have any mechanisms to try and optimize the score, they are a byproduct of the shortest path found. There are instances where another direct path with the same length would result in a better score, as can be seen in an animation by running the cells below. Negative scores are usually the result of direct paths without dots to the goal. Except in those situations, the method had better scores in dense mazes with a far goal because the found path was more likely to have more dots.\n",
    "\n",
    "The second model, by allowing to go through positions again every time a dot is eaten, is a lot slower, but there is a noticeable difference between the results for the dense and sparse variations of the test mazes. A higher percentage of dense mazes timed out compared to sparse mazes. This happens because sparse mazes, by having fewer dots, also \"reset\" the positions fewer times, and so the state space is smaller and goal can be reached more quickly. The amount of visited and explored nodes reflect this behavior.\n",
    "\n",
    "Analysing just the behavior of this method, isolated, it is clear that the model **without the maze in the states** is the superior choice, with the same result but up to thousands of times faster and exploring/visiting fewer nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = np.genfromtxt('mazes/dense/2a', dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.search(breadth_first_graph_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = agent.transform_path()\n",
    "agent.display_path(path, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Maze Animation**\n",
    "Running the cells below, a random maze can be solved with BFS and the first model (since it is better). An animation will run showcasing the solution (in a new window) and an ASCII representation of the solution will also be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.search(breadth_first_graph_search)\n",
    "sol = agent.apply_actions(agent.get_solution())\n",
    "print(sol.astype('<U1'))\n",
    "print(\"Score: \", agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = agent.transform_path()\n",
    "agent.display_path(path, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Depth-First Search Solution**\n",
    "###### **Responsible:** Daniel\n",
    "\n",
    "#### **The Method**\n",
    "The Depth-First Search(DFS) is an Uninformed Search method that expands its deepest node possible first, the one that has no sucessor. If the node is the goal state, it has found the path to the solution, otherwise it goes back to the most recent ancestor node that still has successor that were not expanded and it expands the deepest sucessor this ancestor node has.\n",
    "\n",
    "For the Pacman Problem the goal state considered for the problem was the final position that pacman should go. It did not consider analyzing all the possible outcomes of eating the dots to increase it points because the time and memory to consider all possible states would be exponential.\n",
    "\n",
    "The choosen search method between the two possible types of DFS inside the library was the **depth_first_graph_search**. The **depth_first_tree_search** implementation does not prevent a loop state as the pacman can return to the same position if its reach a dead end. The **depth_first_graph_search**, on the otherhand, avoids repeated states and redundant paths which can solve the problem of pacman getting caught in a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of the DFS using only the indexes as state**\n",
    "\n",
    "As first approach, we used only the position of the Pacman as part of the state of the problem. After running the code we can see the tables to view the time it took to calculate the solution for each maze and the number of points Pacman scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_no_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers DFS method and returns the score '''\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.search(depth_first_graph_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    dfs_no_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/dfs/nomaze'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results of the DFS using the map and indexes as state**\n",
    "\n",
    "In the second approach we also considered the maze including its dots as part of the state. After running the code we can see the tables to view the time it took to calculate the solution for each maze and the number of points Pacman scored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfs_with_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers DFS and returns the score '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.search(depth_first_graph_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    dfs_with_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    "    out_path='data/dfs/maze'\n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DFS with no maze Animation**\n",
    "This a animation using one example of the mazes showing the DFS solution not considering the maze as part of state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = 'mazes/dense/1a'\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.search(depth_first_graph_search)\n",
    "sol = agent.apply_actions(agent.get_solution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = agent.transform_path()\n",
    "agent.display_path(path, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DFS with maze Animation**\n",
    "This a animation using one example of the mazes showing the DFS solution considering the maze as part of state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = 'mazes/dense/1a'\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "agent.search(depth_first_graph_search)\n",
    "sol = agent.apply_actions(agent.get_solution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = agent.transform_path()\n",
    "agent.display_path(path, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the tables of the two possible uses of the DFS, we can see the that using only the index in the state, we achived better results. With only the index as the state of the problem, the algorithm arrived at the solution faster than the other case. This ocurred because when we choose to use the maze as part of the state of the problem, each time the pacman eats a dot it can visit positions it already visited because the state will be diferent from what it was before therefore the DFS considers it a new state. This behavior causes the number of nodes to be visited and expanded much larger than using only the position.\n",
    "\n",
    "We can visualize the behavior of Pacman comparing both animations above. Also, we can see that Pacman ends up scoring poorly compared with the first approach because it ends up repeating the same position too many times making the second solution even more inneficient.\n",
    "\n",
    "With this becomes clear that for the DFS considering the maze as part of the state does not help us achieve a better score or perfomance in the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Maze Animation**\n",
    "Running the cells below, a random maze can be solved with DFS and the first model (since it is faster). An animation will run showcasing the solution (in a new window) and an ASCII representation of the solution will also be available. This availabe to show the solution working on different types of maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.search(depth_first_graph_search)\n",
    "sol = agent.apply_actions(agent.get_solution())\n",
    "print(sol.astype('<U1'))\n",
    "print(\"Score: \", agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = agent.transform_path()\n",
    "agent.display_path(path, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Informed Search Methods**\n",
    "\n",
    "In contrast to the uninformed search methods, the agent in an informed search makes decisions based on additional knowledge about the search space. For this it has to evaluate which is the most promising path to take when it enters a state, which it does by using its knowledge to estimate how far a node is from the goal. Thus, specially for large spaces, the goal can be reached both in less time and more efficiently, depending on the estimation quality. In this project, we were requested to implement two of these evaluations, also known as heuristics.\n",
    "\n",
    "### **Heuristics**\n",
    "\n",
    "A common approach in pathfinding problems is to use the **Manhattan distance** as a heuristic, defined as the distance between the agent and the goal positions measured along axes at right angles (i.e., $|x_1 - x_2| + |y_1 - y_2|$, given that the agent is in $(x_1, y_1)$ and the goal is to reach $(x_2, y_2)$). In our problem, though, it's not a good decision to try to estimate a good cost to goal without taking into account the current configuration knowledge (the combinations of foods can make any estimation over the initial maze very far from optimal). Considering this, and keeping it simple in terms of code, we implemented the following heuristics:\n",
    "\n",
    " - Maximum distance between the agent and a food, as in several cases it's good to reduce the foods reachable area;\n",
    " - Minimum distance between the agent and a food, as the agent will likely approach the nearest food.\n",
    " \n",
    "Notice that both overestimate the optimal - it's not true that approaching the nearest or (notably) the farthest food will increase the agent final score. As overestimating heuristics, they break admissibility - that is, there is no guarantee that the optimal path will be found in the informed search algorithms. Even so, as our problem gives a high score to Pac-Man when it eats, we chose both expecting good paths to be found in reasonable running times.\n",
    "\n",
    "We also take the goal into account as a \"food\" in these heuristics after a certain depth, to avoid some situations in sparse mazes where the agent thinks it's better to pick a food far from it's position than going directly to goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_type = ''\n",
    "def min_or_max_manhattan(node, maze = None):\n",
    "    ''' minimum/maximum distance between the agent and a food '''\n",
    "        \n",
    "    assert h_type != '', 'Define a heuristic type (min or max)!'\n",
    "        \n",
    "    # Detach maze configuration (if in state) and Pac-Man position\n",
    "    if not isinstance(maze, np.ndarray):\n",
    "        tuple_maze, idx = node.state\n",
    "    else:\n",
    "        idx = node.state\n",
    "        tuple_maze = None\n",
    "    \n",
    "    # Convert tuple maze into a numpy array\n",
    "    maze = np.array(tuple_maze) if tuple_maze else maze\n",
    "    \n",
    "    # Take goal into account after a certain depth\n",
    "    if node.depth < maze.shape[0]*maze.shape[1]*(3/4):\n",
    "        foods_cond = maze == b'.'\n",
    "    else:\n",
    "        foods_cond = np.logical_or(maze == b'.', maze == b'?')\n",
    "    \n",
    "    # List all manhattan distances between the agent and the foods\n",
    "    manhattan_distances = [\n",
    "        manhattan_distance(food_idx, idx) \n",
    "        for food_idx \n",
    "        in np.argwhere(foods_cond)\n",
    "    ]\n",
    "    \n",
    "    # If no more dots.\n",
    "    if len(manhattan_distances) == 0:\n",
    "        manhattan_distances = [\n",
    "            manhattan_distance(food_idx, idx) \n",
    "            for food_idx \n",
    "            in np.argwhere(maze == b'?')\n",
    "        ]\n",
    "    \n",
    "    # Get lowest/highest of them\n",
    "    return min(manhattan_distances) if h_type == 'min' else max(manhattan_distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A* Search Solution**\n",
    "###### **Responsible:** Eduardo\n",
    "\n",
    "As an informed search algorithm, A\\* considers information about the exact path cost $g(n)$ from starting node to $n$ together with an heuristic $h(n)$ to estimate the total cost to goal, proceeding to the neighbor that gives the lowest $f(n) = g(n) + h(n)$. Because it balances accurate past information with a context-flexible estimation, it's one of the most adopted search algorithms in software system such as games, where character navigation through complex stages is a common problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for problem without maze in state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Manhattan min' heuristic\n",
    "h_type = 'min'\n",
    "def astar_min_nomaze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(min_or_max_manhattan)\n",
    "    agent.search(astar_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    astar_min_nomaze, \n",
    "    [], \n",
    "    repeat=1, \n",
    ")\n",
    "\n",
    "for df in dataframes[4:]:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Manhattan max' heuristic\n",
    "h_type = 'max'\n",
    "def astar_max_nomaze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(min_or_max_manhattan)\n",
    "    agent.search(astar_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    astar_max_nomaze, \n",
    "    [], \n",
    "    repeat=1, \n",
    ")\n",
    "\n",
    "for df in dataframes[4:]:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for problem with maze in state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Manhattan min' heuristic\n",
    "h_type = 'min'\n",
    "def astar_min_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(min_or_max_manhattan)\n",
    "    agent.search(astar_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    astar_min_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    ")\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Manhattan max' heuristic\n",
    "h_type = 'max'\n",
    "def astar_max_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(min_or_max_manhattan)\n",
    "    agent.search(astar_search)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "    test_files, \n",
    "    astar_max_maze, \n",
    "    [], \n",
    "    repeat=1, \n",
    ")\n",
    "\n",
    "for df in dataframes[4:]:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis**\n",
    "\n",
    "As expected, A* turns out to perform really well in both models, with emphasis on the second. In that, the agent generally find a path with a higher than 1000 score (the equivalent of 100 foods eaten) in less than a second, which is very impressive considering how big the search space is. But even in the first model, where the search space is quadratic, it can almost score 1000 in average.\n",
    "\n",
    "Unlike in other methods, the second problem is a justifiable choice in this one. With the non admissible heuristics, A* prune a lot of nodes, as it concludes they won't improve the solution. By running a random animation in the cell below you can see why that's not always true - Pac-Man occasionally fails to make decisions that would increase it's score. This reflects our attempt to sacrifice optimality (while still obtaining great paths) for the sake of running time.\n",
    "\n",
    "#### **Random Maze Animation**\n",
    "\n",
    "Running the cells below, a random maze can be solved with A* and the second model (since it is the most interesting to see in action for this model). An animation will run showcasing the solution (in a new window) and an ASCII representation of the solution will also be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "h_type = 'min'\n",
    "agent.set_heuristic(min_or_max_manhattan)\n",
    "agent.search(astar_search)\n",
    "sol = agent.apply_actions(agent.get_solution())\n",
    "print(sol.astype('<U1'))\n",
    "print(\"Score: \", agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = agent.transform_path()\n",
    "agent.display_path(path, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Greedy Best-First Search (GS) Solution**\n",
    "###### **Responsible:** Victor and Eduardo\n",
    "\n",
    "#### **The Method**\n",
    "\n",
    "Greedy Best-First Search, or Greedy Search (GS) is an informed search method that visits a state based on a greedy criterion that tries to decide the best path to follow. This greedy criterion is determined by the chosen heuristic, and **only** it. \n",
    "\n",
    "So the difference between GS and $A^*$ is that, while the latter uses the **path cost** along with the heuristic value, the former only considers the heuristic value. Aside from that, the implementation of both methods in _AIMA_ are exactly the same.\n",
    "\n",
    "GS, for the Pac-Man problem, is a complete method, since the maze is finite and the represents the problem in **graph** form, so the method does not visit the same state twice. The graph variation is used because there can be loops in the state space, so a tree is not the best representation of it. It is **not optimal**, though, for the greedy decision might not lead to the best overall solution, which depends on the distribution of dots and goal position.\n",
    "\n",
    "Since the method doesn't expand nodes out of the way of the solution, the time (and therefore, memory) complexity is, in practice, lower than the complexity of the uninformed methods, but in the worst case it can be reduced to DFS, since it returns after finding a dead-end.\n",
    "\n",
    "This method is **deterministic**, so it will **always** return the same solution to an instance. For that reason, there's no use in averaging results from different executions, and the method can be analyzed with only one execution.\n",
    "\n",
    "The _AIMA_ library does not have a wrapper for this method, unlike for $A^*$, to help with heuristic organization and first memoization (_problem.h_ or parameter h). For that reason, a wrapper quite like the one for $A^*$ in _AIMA_ was made, and is available in the below cell.\n",
    "\n",
    "#### **Pre-Analysis**\n",
    "\n",
    "Since GS does not include the path cost in its choice, it will have less issues going through empty spaces, which will make its solution potentially different from $A^*$, but not necessarily better or worse. In other aspects, it will likely have the same behavior as the previous method, going after most dots in the map before going to the goal, trying to optimize the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gfs_wrapper(problem, h=None):\n",
    "    h = memoize(h or problem.h, 'h')\n",
    "    return greedy_best_first_graph_search(problem, lambda n: h(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for problem without maze in state**\n",
    "Both heuristics will be used in both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Manhattan min' heuristic\n",
    "h_type = 'min'\n",
    "def gfs_min_nomaze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(min_or_max_manhattan)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_min_nomaze, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        )\n",
    "\n",
    "for df in dataframes[4:]:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Manhattan max' heuristic\n",
    "h_type = 'max'\n",
    "def gfs_max_nomaze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(min_or_max_manhattan)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests \n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_max_nomaze, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        )\n",
    "\n",
    "for df in dataframes[4:]:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results for problem with maze in state**\n",
    "\n",
    "CAUTION: the below cell can take a really long time to run, and use a lot of RAM if timeout is too slow or not working. Timeout **does not work on Windows**, so if that's the used operating system, take out/comment that line, and be careful with RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Manhattan min' heuristic\n",
    "h_type = 'min'\n",
    "@timeout(60)\n",
    "def gfs_min_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(min_or_max_manhattan)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_min_maze, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        )\n",
    "\n",
    "for df in dataframes[4:]:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Manhattan max' heuristic\n",
    "h_type = 'max'\n",
    "@timeout(60)\n",
    "def gfs_max_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(min_or_max_manhattan)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_max_maze, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        )\n",
    "\n",
    "for df in dataframes[4:]:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analysis**\n",
    "\n",
    "The heuristics described didn't worked as we expected in this method, particularly in the second problem. As the value doesn't take the path cost into account, the agent can't infer well enough if going to the right is better than going to the left, for instance - in a lot of cases the estimation will be the same. Even for the model without maze in state, the results weren't satisfactory. To analyse it better, we tried a simple \"manhattan distance between the agent and the goal\" heuristic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_goal(node, maze = None):\n",
    "    ''' manhattan distance between the agent and the goal '''\n",
    "                \n",
    "    tuple_maze, idx = node.state\n",
    "    maze = np.array(tuple_maze)\n",
    "    goal_idx = np.argwhere(maze == b'?')[0]\n",
    "    return manhattan_distance(goal_idx, idx)\n",
    "\n",
    "# Problem with maze in state, which didn't worked before\n",
    "@timeout(10)\n",
    "def gfs_goal_maze(agent, maze, init, goal, *args):\n",
    "    ''' triggers A* search and returns score '''\n",
    "    agent.formulate_problem(init, goal, True, [b'-', b'|', b'o', b'_'])\n",
    "    agent.set_heuristic(manhattan_goal)\n",
    "    agent.search(gfs_wrapper)\n",
    "    return agent.get_score()\n",
    "\n",
    "# Run all tests\n",
    "dataframes = run_tests(\n",
    "        test_files, \n",
    "        gfs_goal_maze, \n",
    "        [], \n",
    "        repeat=1, \n",
    "        )\n",
    "\n",
    "for df in dataframes[4:]:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results weren't that good, but it ran really fast - which is a sign that a way to improve our heuristics would be to give a higher weight to the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Maze Animation**\n",
    "Running the cells below, a random maze can be solved with GFS using the first model and the _manhattan min_ heuristic, since it gave the best results. An animation will run showcasing the solution (in a new window). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "h_type = 'min'\n",
    "agent.set_heuristic(min_or_max_manhattan)\n",
    "agent.search(gfs_wrapper)\n",
    "sol = agent.apply_actions(agent.get_solution())\n",
    "print(sol.astype('<U1'))\n",
    "print(\"Score: \", agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = agent.transform_path()\n",
    "agent.display_path(path, 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Local Search Methods**\n",
    "Local search is one of many heuristic based methods for optmization problems. They work by exploring the search space taking its huristic in consideration, the solution is given by memorizing the positions visited when executing such exploration. Usually, the search is done on feasible solutions, but, in our case, the search is used in a manner which it helps us find the feasible solution by iterating throught several intermediate states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simulated Annealing (SA) Solution**\n",
    "###### **Responsible:** Vinicius\n",
    "Simulated Annealing resembles Hillclimbing, but it enchances it to avoid local optimums. It does so by allowing steps which contradict the huristics decision: in hillclimbing we always take the best local decision hoping it will lead us to a global optimum, in simulated annealing we allow (with a probability which decreases over time) the search to take locally worst steps hoping it will allow us to scape local optimums. \n",
    "\n",
    "AIMA's implementation chooses at random the next node from a list of possible neighboors. Overall, it first selects a possible neighbor at random, then it evaluates the neighboors quality using the diference between the heuristic result for the current state and the neighbooring state. If the neighboor presents itself as a better state, the step is certainly taken, however, if is a worst state, the step is taken with a given probability `p`.\n",
    "\n",
    "Simulated Annealing is a **stochastic method**. It **does not ensure completeness** as it can re-visit states some states and never visit others. The method also does not ensure optmality of a found path.\n",
    "\n",
    "#### **Heuristics**\n",
    "We choose several heuristics to be used with this method.\n",
    "\n",
    "**Path Cost Difference (P):** Here we consider the diference of costs among states. If the next state has a lower cost than the current state, then it's considered a better state. It's important to notice that this heuristic does not contain any information about the proximity to the goal. \n",
    "\n",
    "**Euclidean Distance to Goal (E):** it consists of calculating the euclidean distance of a state to the goal. We calculate the diference between the distance from the current state to the goal and the distance from the next state to the goal. If the distance on the next state is smalller the current state, then the next state is better.\n",
    "\n",
    "**Manhatam Distance to Goal (M):** in this case, we'll also use the diference between distances to the goal as a heuristic, but the manhatam distance is caluclated using the horizontal and vertical distance $(|Xa - Xb| + |Yz - Yb|)$. Then the same as before applies: smaller the distance to the goal, the better the state is considered.\n",
    "\n",
    "To use the Path Cost heuristic we must keep the current state cost, however, this does not need to be part of the state itself. The remaining heuristics only requires the coordinates of the current position and the goal position. The PacProblemNoMaze was used for all of the heuristics tests, simply because including the maze in the state would be utterly pointless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Temperature Function Parameters**\n",
    "\n",
    "The function responsible for controling the probability `p` takes three parameters: the initial temperature `K`, the rate of cooling `lamb` and the maximum ammount of steps `limit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cooling function for simulated annealing\n",
    "def cooler(k=5, lam=0.0003, limit=3000):\n",
    "    \"\"\"One possible schedule function for simulated annealing\"\"\"\n",
    "    return lambda t: (k * np.exp(-lam * t) if t < limit else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters were choosen by taking these considerations:\n",
    " - The P heuristic will always result in either -1 or 9, because we either dont consume a dot or consume exactly one. To balance the cooling function starting probability we defined `k=5`. This implies that the initial probability of taking a worst step is $\\exp{\\frac{-1}{5}} \\approx 0.82$, which is reasonable.\n",
    " - The value for `limit` was tested empirically. Three thousand presented a reasonable ammount of time and memory usage with ammount of explored space. \n",
    " - The value for `lamb` was tested empirically and a initial estimate was based dividing `K` by `limit`, then it as adjusted manually as it was taking too long to cooldown the search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Post Processing Function**\n",
    "A important detail for this method is the fact that it requires a post processing step. The AIMA's implementation does not check for the goal state. The execution terminates only when the limit of iterations is reached. Therefore, in the end we have a list of states in which we must manually check for the goal state and score. We recalculate the scores for each state and filter which the states that are the same as the goal state and have the lowest cost. Finally, we slice the list at the best goal state and return the section remaining as the best path found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_path(states, maze, goal):\n",
    "        ''' Calculate the cost after getting a path. '''\n",
    "        min_idx = None \n",
    "        min_cost = sys.maxsize\n",
    "        cost = 0\n",
    "\n",
    "        for i,pos in enumerate(states):        \n",
    "            if maze[pos] == b'.':\n",
    "                cost -= 9 # Reduce cost by 10 and add 1\n",
    "                maze[pos] = b' ' # Eat pos\n",
    "            else:\n",
    "                cost += 1\n",
    "\n",
    "            # Check if at a goal state with a better cost\n",
    "            if pos == goal and cost < min_cost:\n",
    "                min_cost = cost\n",
    "                min_idx = i\n",
    "\n",
    "        # Check if reached the goal state at all\n",
    "        if min_idx:\n",
    "            path = states[:min_idx+1]\n",
    "            return (path, min_cost)\n",
    "        else: \n",
    "            return (states, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Wrapper Function**\n",
    "This function is responsible for receving the SA parameters, executing the search and then the post processing function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annealing(problem, maze, goal, heuristic):\n",
    "    # Copy for post processing\n",
    "    maze_ref = maze.copy()\n",
    "\n",
    "    # Define Heuristic\n",
    "    problem.heuristic = heuristic\n",
    "\n",
    "    # Solve with Simulated Annealing\n",
    "    states = simulated_annealing_full(problem, schedule=cooler())\n",
    "\n",
    "    # Get the best path found\n",
    "    best = get_best_path(states, maze_ref, goal)\n",
    "    \n",
    "    return best    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be responsible for initializing the agent which will call the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(agent, maze, init, goal, *args):\n",
    "    agent.formulate_problem(init, goal, False, [])\n",
    "    agent.search(annealing, maze, goal, *args[0])\n",
    "    return agent.get_solution()[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Path Cost Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = run_tests(\n",
    "                test_files, \n",
    "                simulated_annealing, \n",
    "                ['pathcost'], \n",
    "                repeat=100, \n",
    "                )\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immediate Analysis:** \n",
    "\n",
    "The first noticeble aspect is the considerable percentage of faillure on the P heurisitic. It's comprehensible, howerver, since the heuristic itself computes nothing related to the goal itself, but only the cost attained by the path. \n",
    "\n",
    "It's noticeable the difference ammong the sparse and dense mazes score. The initial behaviour for dense mazes is reasonable, but, as the dots are consumed, it starts to take to many bad decisions without any notion of where the remaining dots might be (since it only considers neighboors positions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Euclidean Distance Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = run_tests(\n",
    "                test_files, \n",
    "                simulated_annealing, \n",
    "                ['euclidean'], \n",
    "                repeat=100, \n",
    "                )\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immediate Analysis:** \n",
    "\n",
    "There's a considerable improvement in the fails ratio for the E heuristic. This is justifiable as it's only consideration is reaching the goal. Consequentially, it also presents a far better score than the P heuristic, which is also expected since it will avoid many unnecessary steps until reaching the goal. Even so, we hoped for a better score in the dense mazes.\n",
    "\n",
    "It's also noticeable the increase in execution time. It's likely due to the overhead of calling and executing the euclidean distance function, given that the SA search always execute the ammount of steps impose by the `limit` variable in the cooling function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Manhattan Distance Heuristic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = run_tests(   test_files, \n",
    "                          simulated_annealing, \n",
    "                          ['manhattan'], \n",
    "                          repeat=100)\n",
    "\n",
    "for df in dataframes:\n",
    "    print(f'================ {df.name} ================\\n')\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immediate Analysis:** \n",
    "\n",
    "Oddly, the M heuristic presented an increased number of failures for the presented mazes. It would be expected to be closer to the euclidean distance, however it had a failure rate of more than double of the rate for the E huristic.\n",
    "\n",
    "The execution time remained about the same as the E heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Random Maze Animation**\n",
    "Running the cell below, a random maze can be solved with simulated annealing using the first model and the euclidean heuristic, since it gave the least failures. An animation will run showcasing the solution (in a new window). To change models, just change the _True_ flag in the _formulate_problem_ function call. To change heuristic, just change the argument in the _set_heuristic_ function call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_file = choice(test_files)\n",
    "maze = np.genfromtxt(maze_file, dtype=str, delimiter=1).astype('bytes')\n",
    "agent.set_maze(maze)\n",
    "init, goal = agent.find_positions()\n",
    "agent.formulate_problem(init, goal, False, [b'-', b'|', b'o', b'_'])\n",
    "agent.set_heuristic('euclidean')\n",
    "agent.search(annealing)\n",
    "print(\"Score: \", -1*agent.get_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = agent.transform_path()\n",
    "agent.display_path(path, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparisons**\n",
    "\n",
    "> TODO List:\n",
    "- [X] Compare methods and problems\n",
    "- [ ] Maybe display a graph comparing scores\n",
    "\n",
    "The results show that the tested methods are very different from one another, leading to vastly different results, some horrible and some great.\n",
    "\n",
    "As expected, the uninformed methods were far from the best in score since they, as the name suggests, do not use any information of the problem to make a decision regarding the next node to visit. DFS, in particular, represents this problem very well, leading to very low negative scores in the larger state space model. BFS, on the other hand, gets the shortest path to the goal, so no movements are \"wasted\", but it doesn't give a very high score either, so this might indicate that those are not good methods for the Pac-Man problem.\n",
    "\n",
    "The informed methods, on the other hand, returned better results, focusing on improving the score. The main problem with those methods can be the goal: contrary to the uninformed methods, the heuristics used for these methods **do not** take priority on the goal as the primary objective, instead going after more dots for higher score. This, in its worst case, can lead to very high time/memory requirements and a high amount of explored nodes, since the execution only stops after reaching the goal, as can be seen in the GFS with the second model. The $A^*$ method is better than this since it also considers the path cost, so it hardly takes a long path without dots unless it is worth it. This prunes the state space more, leading to faster solutions, with less \"wasted\" movements. So, overall, these are the most interesting methods for the problem if the objective is to maximize score.\n",
    "\n",
    "TODO: WRONG\n",
    "\n",
    "The simulated annealing method had more interesting results since it, contrary to the other methods, can repeat the same state, it doesn't have a graph representation. This method can have varying levels of success depending on the objective function used. The main problems with this method are the amount of failures and \"wasted movements\", reducing the score and leading to many more repeated states than explored ones. It also fared better when it was less overwhelmed with possibility, in the sparse mazes. Overall, this method can seem unstable and prone to failure, but it can (with a certain probability) return a good (or even great) result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusions**\n",
    "\n",
    "### **Models**\n",
    "It is visible that different methods return better solutions for different models. Some methods are almost impractible with the more powerful model, as is the case with BFS and GFS, while others take full advantage of that version ($A^*$). DFS is a special case, that doesn't explode as much as some others, but gives some horrible solutions.\n",
    "\n",
    "Meanwhile, the local search method with the specified value calculations does not diferentiate both models, so any of them could be used.\n",
    "\n",
    "### **Methods**\n",
    "After comparing every method, the one that performed in terms of solution cost, time/memory requirements and failures was the $A^*$ method. This result makes sense, since an informed method with good heuristics can maximize the score more easily than other types, and using the path cost with the heuristic value guarantees that the number of steps is taken into account. For those reasons, $A^*$ is often used for 2D pathfinding problems, even more than the other methods.\n",
    "\n",
    "The best heuristic for $A^*$ was _manhattan_min_, so....\n",
    "\n",
    "Therefore, the overall best solution is given by the $A^*$ method, using the _manhattan_min_ heuristic in the second, most powerful model. The execution time is still very low compared to others, and on average gives better results in score.\n",
    "\n",
    "The uninformed methods were not very suitable for the task, since they do not focus on the score, which is used to measure quality. \n",
    "\n",
    "TODO: WRONG\n",
    "\n",
    "The local search, while in theory good for optimization problems, in practice can waste too many points in \"wasted movements\". The heuristic related to path cost was the one to return better results, which is a similar procedure to an informed method such as $A^*$, but overall is an unstable method with high degree of failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Future Work**\n",
    "As mentioned near the beginning of this report, the two models created are not the only ways to model the problem. As examples, other models could include adding the amount of dots left in the maze instead of the entire maze, keeping the maze as a problem attribute, or adding a maze/amount of dots in the goal state so that another specific task is achieved, such as eat all the dots in the maze (the goal state would be the position and an empty maze, or 0), and the list keeps going.\n",
    "\n",
    "The goal, in this project, was used to end the execution, but it did not have a different value or had a higher priority than any dot, therefore the priority for the problem was always the score, for those methods that try to optimize it. Other ways to model the problem could include giving more importance to the goal, so methods such as GFS would search for the goal instead of more ways to increase the score (see, for example, the goal-based heuristic for GFS).\n",
    "\n",
    "Future work could include analysing those other modeling ideas, and **check memory usage more precisely**, for an even better discussion. Online methods and genetic algorithm-based solutions could also be implemented, since they were not even touched upon in this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
